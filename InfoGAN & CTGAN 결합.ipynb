{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOeidyeqfVTxOIIyq7iwIiv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# CTGAN과 같은 모델에서 기본적으로 사용되는 기능들을 제공하는 함수들"],"metadata":{"id":"ciMSkNvvzIle"}},{"cell_type":"code","source":["\"\"\"BaseSynthesizer module.\"\"\"\n","\n","import contextlib\n","import numpy as np\n","import torch\n","\n","\n","@contextlib.contextmanager\n","def set_random_states(random_state, set_model_random_state):\n","    \"\"\"Context manager for managing the random state.\n","\n","    Args:\n","        random_state (int or tuple):\n","            The random seed or a tuple of (numpy.random.RandomState, torch.Generator).\n","        set_model_random_state (function):\n","            Function to set the random state on the model.\n","    \"\"\"\n","    original_np_state = np.random.get_state()\n","    original_torch_state = torch.get_rng_state()\n","\n","    random_np_state, random_torch_state = random_state\n","\n","    np.random.set_state(random_np_state.get_state())\n","    torch.set_rng_state(random_torch_state.get_state())\n","\n","    try:\n","        yield\n","    finally:\n","        current_np_state = np.random.RandomState()\n","        current_np_state.set_state(np.random.get_state())\n","        current_torch_state = torch.Generator()\n","        current_torch_state.set_state(torch.get_rng_state())\n","        set_model_random_state((current_np_state, current_torch_state))\n","\n","        np.random.set_state(original_np_state)\n","        torch.set_rng_state(original_torch_state)\n","\n","\n","def random_state(function):\n","    \"\"\"Set the random state before calling the function.\n","\n","    Args:\n","        function (Callable):\n","            The function to wrap around.\n","    \"\"\"\n","\n","    def wrapper(self, *args, **kwargs):\n","        if self.random_states is None:\n","            return function(self, *args, **kwargs)\n","\n","        else:\n","            with set_random_states(self.random_states, self.set_random_state):\n","                return function(self, *args, **kwargs)\n","\n","    return wrapper\n","\n","\n","class BaseSynthesizer:\n","    \"\"\"Base class for all default synthesizers of ``CTGAN``.\"\"\"\n","\n","    random_states = None\n","\n","    def __getstate__(self):\n","        \"\"\"Improve pickling state for ``BaseSynthesizer``.\n","\n","        Convert to ``cpu`` device before starting the pickling process in order to be able to\n","        load the model even when used from an external tool such as ``SDV``. Also, if\n","        ``random_states`` are set, store their states as dictionaries rather than generators.\n","\n","        Returns:\n","            dict:\n","                Python dict representing the object.\n","        \"\"\"\n","        device_backup = self._device\n","        self.set_device(torch.device('cpu'))\n","        state = self.__dict__.copy()\n","        self.set_device(device_backup)\n","        if (\n","            isinstance(self.random_states, tuple) and\n","            isinstance(self.random_states[0], np.random.RandomState) and\n","            isinstance(self.random_states[1], torch.Generator)\n","        ):\n","            state['_numpy_random_state'] = self.random_states[0].get_state()\n","            state['_torch_random_state'] = self.random_states[1].get_state()\n","            state.pop('random_states')\n","\n","        return state\n","\n","    def __setstate__(self, state):\n","        \"\"\"Restore the state of a ``BaseSynthesizer``.\n","\n","        Restore the ``random_states`` from the state dict if those are present and then\n","        set the device according to the current hardware.\n","        \"\"\"\n","        if '_numpy_random_state' in state and '_torch_random_state' in state:\n","            np_state = state.pop('_numpy_random_state')\n","            torch_state = state.pop('_torch_random_state')\n","\n","            current_torch_state = torch.Generator()\n","            current_torch_state.set_state(torch_state)\n","\n","            current_numpy_state = np.random.RandomState()\n","            current_numpy_state.set_state(np_state)\n","            state['random_states'] = (\n","                current_numpy_state,\n","                current_torch_state\n","            )\n","\n","        self.__dict__ = state\n","        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        self.set_device(device)\n","\n","    def save(self, path):\n","        \"\"\"Save the model in the passed `path`.\"\"\"\n","        device_backup = self._device\n","        self.set_device(torch.device('cpu'))\n","        torch.save(self, path)\n","        self.set_device(device_backup)\n","\n","    @classmethod\n","    def load(cls, path):\n","        \"\"\"Load the model stored in the passed `path`.\"\"\"\n","        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        model = torch.load(path)\n","        model.set_device(device)\n","        return model\n","\n","    def set_random_state(self, random_state):\n","        \"\"\"Set the random state.\n","\n","        Args:\n","            random_state (int, tuple, or None):\n","                Either a tuple containing the (numpy.random.RandomState, torch.Generator)\n","                or an int representing the random seed to use for both random states.\n","        \"\"\"\n","        if random_state is None:\n","            self.random_states = random_state\n","        elif isinstance(random_state, int):\n","            self.random_states = (\n","                np.random.RandomState(seed=random_state),\n","                torch.Generator().manual_seed(random_state),\n","            )\n","        elif (\n","            isinstance(random_state, tuple) and\n","            isinstance(random_state[0], np.random.RandomState) and\n","            isinstance(random_state[1], torch.Generator)\n","        ):\n","            self.random_states = random_state\n","        else:\n","            raise TypeError(\n","                f'`random_state` {random_state} expected to be an int or a tuple of '\n","                '(`np.random.RandomState`, `torch.Generator`)')"],"metadata":{"id":"wh2sLImPzHsx","executionInfo":{"status":"ok","timestamp":1716703000529,"user_tz":-540,"elapsed":1759,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# DataSampler 함수 (이산형 변수 랜덤하게 선택한 후 로그확률로 카테고리 선택)"],"metadata":{"id":"x-UGf5A1rZfQ"}},{"cell_type":"code","source":["\"\"\"DataSampler module.\"\"\"\n","\n","import numpy as np\n","\n","\n","class DataSampler(object):\n","    \"\"\"DataSampler samples the conditional vector and corresponding data for CTGAN.\"\"\"\n","\n","    def __init__(self, data, output_info, log_frequency):\n","        self._data_length = len(data)\n","\n","        def is_discrete_column(column_info):\n","            return (len(column_info) == 1\n","                    and column_info[0].activation_fn == 'softmax')\n","\n","        n_discrete_columns = sum(\n","            [1 for column_info in output_info if is_discrete_column(column_info)])\n","\n","        self._discrete_column_matrix_st = np.zeros(\n","            n_discrete_columns, dtype='int32')\n","\n","        # Store the row id for each category in each discrete column.\n","        # For example _rid_by_cat_cols[a][b] is a list of all rows with the\n","        # a-th discrete column equal value b.\n","        self._rid_by_cat_cols = []\n","\n","        # Compute _rid_by_cat_cols\n","        st = 0\n","        for column_info in output_info:\n","            if is_discrete_column(column_info):\n","                span_info = column_info[0]\n","                ed = st + span_info.dim\n","\n","                rid_by_cat = []\n","                for j in range(span_info.dim):\n","                    rid_by_cat.append(np.nonzero(data[:, st + j])[0])\n","                self._rid_by_cat_cols.append(rid_by_cat)\n","                st = ed\n","            else:\n","                st += sum([span_info.dim for span_info in column_info])\n","        assert st == data.shape[1]\n","\n","        # Prepare an interval matrix for efficiently sample conditional vector\n","        max_category = max([\n","            column_info[0].dim\n","            for column_info in output_info\n","            if is_discrete_column(column_info)\n","        ], default=0)\n","\n","        self._discrete_column_cond_st = np.zeros(n_discrete_columns, dtype='int32')\n","        self._discrete_column_n_category = np.zeros(n_discrete_columns, dtype='int32')\n","        self._discrete_column_category_prob = np.zeros((n_discrete_columns, max_category))\n","        self._n_discrete_columns = n_discrete_columns\n","        self._n_categories = sum([\n","            column_info[0].dim\n","            for column_info in output_info\n","            if is_discrete_column(column_info)\n","        ])\n","\n","        st = 0\n","        current_id = 0\n","        current_cond_st = 0\n","        for column_info in output_info:\n","            if is_discrete_column(column_info):\n","                span_info = column_info[0]\n","                ed = st + span_info.dim\n","                category_freq = np.sum(data[:, st:ed], axis=0)\n","                if log_frequency:\n","                    category_freq = np.log(category_freq + 1)\n","                category_prob = category_freq / np.sum(category_freq)\n","                self._discrete_column_category_prob[current_id, :span_info.dim] = category_prob\n","                self._discrete_column_cond_st[current_id] = current_cond_st\n","                self._discrete_column_n_category[current_id] = span_info.dim\n","                current_cond_st += span_info.dim\n","                current_id += 1\n","                st = ed\n","            else:\n","                st += sum([span_info.dim for span_info in column_info])\n","\n","    def _random_choice_prob_index(self, discrete_column_id):\n","        probs = self._discrete_column_category_prob[discrete_column_id]\n","        r = np.expand_dims(np.random.rand(probs.shape[0]), axis=1)\n","        return (probs.cumsum(axis=1) > r).argmax(axis=1)\n","\n","    def sample_condvec(self, batch):\n","        \"\"\"Generate the conditional vector for training.\n","\n","        Returns:\n","            cond (batch x #categories):\n","                The conditional vector.\n","            mask (batch x #discrete columns):\n","                A one-hot vector indicating the selected discrete column.\n","            discrete column id (batch):\n","                Integer representation of mask.\n","            category_id_in_col (batch):\n","                Selected category in the selected discrete column.\n","        \"\"\"\n","        if self._n_discrete_columns == 0:\n","            return None\n","\n","        discrete_column_id = np.random.choice(\n","            np.arange(self._n_discrete_columns), batch)\n","\n","        cond = np.zeros((batch, self._n_categories), dtype='float32')\n","        mask = np.zeros((batch, self._n_discrete_columns), dtype='float32')\n","        mask[np.arange(batch), discrete_column_id] = 1\n","        category_id_in_col = self._random_choice_prob_index(discrete_column_id)\n","        category_id = (self._discrete_column_cond_st[discrete_column_id] + category_id_in_col)\n","        cond[np.arange(batch), category_id] = 1\n","\n","        return cond, mask, discrete_column_id, category_id_in_col\n","\n","    def sample_original_condvec(self, batch):\n","        \"\"\"Generate the conditional vector for generation use original frequency.\"\"\"\n","        if self._n_discrete_columns == 0:\n","            return None\n","\n","        category_freq = self._discrete_column_category_prob.flatten()\n","        category_freq = category_freq[category_freq != 0]\n","        category_freq = category_freq / np.sum(category_freq)\n","        col_idxs = np.random.choice(np.arange(len(category_freq)), batch, p=category_freq)\n","        cond = np.zeros((batch, self._n_categories), dtype='float32')\n","        cond[np.arange(batch), col_idxs] = 1\n","\n","        return cond\n","\n","    def sample_data(self, data, n, col, opt):\n","        \"\"\"Sample data from original training data satisfying the sampled conditional vector.\n","\n","        Args:\n","            data:\n","                The training data.\n","        Returns:\n","            n:\n","                n rows of matrix data.\n","        \"\"\"\n","        if col is None:\n","            idx = np.random.randint(len(data), size=n)\n","            return data[idx]\n","\n","        idx = []\n","        for c, o in zip(col, opt):\n","            idx.append(np.random.choice(self._rid_by_cat_cols[c][o]))\n","\n","        return data[idx]\n","\n","    def dim_cond_vec(self):\n","        \"\"\"Return the total number of categories.\"\"\"\n","        return self._n_categories\n","\n","    def generate_cond_from_condition_column_info(self, condition_info, batch):\n","        \"\"\"Generate the condition vector.\"\"\"\n","        vec = np.zeros((batch, self._n_categories), dtype='float32')\n","        id_ = self._discrete_column_matrix_st[condition_info['discrete_column_id']]\n","        id_ += condition_info['value_id']\n","        vec[:, id_] = 1\n","        return vec"],"metadata":{"id":"9nR_-H5-rTwb","executionInfo":{"status":"ok","timestamp":1716703000530,"user_tz":-540,"elapsed":11,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# 연속형 변수에 대해서 GMM으로 [-1.1]로 정규화 한 후 Tanh로 변환\n","# 이산형 변수에 대해서 Onehotencoder사용해 원-핫 벡터 사용"],"metadata":{"id":"2ZnhCns5vK6s"}},{"cell_type":"code","source":["!pip install rdt"],"metadata":{"id":"9O_EBEUp71gf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716703009209,"user_tz":-540,"elapsed":8688,"user":{"displayName":"허주혁","userId":"07084138917445903099"}},"outputId":"e24be962-a2e5-4704-cbcf-6c0e18b175e3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rdt in /usr/local/lib/python3.10/dist-packages (1.12.1)\n","Requirement already satisfied: Faker>=17 in /usr/local/lib/python3.10/dist-packages (from rdt) (25.2.0)\n","Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from rdt) (2.0.3)\n","Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from rdt) (1.2.2)\n","Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.10/dist-packages (from rdt) (1.25.2)\n","Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from rdt) (1.11.4)\n","Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from Faker>=17->rdt) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->rdt) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->rdt) (2024.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->Faker>=17->rdt) (1.16.0)\n"]}]},{"cell_type":"code","source":["\"\"\"DataTransformer module.\"\"\"\n","\n","from collections import namedtuple\n","\n","import numpy as np\n","import pandas as pd\n","from joblib import Parallel, delayed\n","from rdt.transformers import ClusterBasedNormalizer, OneHotEncoder\n","\n","SpanInfo = namedtuple('SpanInfo', ['dim', 'activation_fn'])\n","ColumnTransformInfo = namedtuple(\n","    'ColumnTransformInfo', [\n","        'column_name', 'column_type', 'transform', 'output_info', 'output_dimensions'\n","    ]\n",")\n","\n","\n","class DataTransformer(object):\n","    \"\"\"Data Transformer.\n","\n","    Model continuous columns with a BayesianGMM and normalize them to a scalar between [-1, 1]\n","    and a vector. Discrete columns are encoded using a OneHotEncoder.\n","    \"\"\"\n","\n","    def __init__(self, max_clusters=10, weight_threshold=0.005):\n","        \"\"\"Create a data transformer.\n","\n","        Args:\n","            max_clusters (int):\n","                Maximum number of Gaussian distributions in Bayesian GMM.\n","            weight_threshold (float):\n","                Weight threshold for a Gaussian distribution to be kept.\n","        \"\"\"\n","        self._max_clusters = max_clusters\n","        self._weight_threshold = weight_threshold\n","\n","    def _fit_continuous(self, data):\n","        \"\"\"Train Bayesian GMM for continuous columns.\n","\n","        Args:\n","            data (pd.DataFrame):\n","                A dataframe containing a column.\n","\n","        Returns:\n","            namedtuple:\n","                A ``ColumnTransformInfo`` object.\n","        \"\"\"\n","        column_name = data.columns[0]\n","        gm = ClusterBasedNormalizer(\n","            missing_value_generation='from_column',\n","            max_clusters=min(len(data), self._max_clusters),\n","            weight_threshold=self._weight_threshold\n","        )\n","        gm.fit(data, column_name)\n","        num_components = sum(gm.valid_component_indicator)\n","\n","        return ColumnTransformInfo(\n","            column_name=column_name, column_type='continuous', transform=gm,\n","            output_info=[SpanInfo(1, 'tanh'), SpanInfo(num_components, 'softmax')],\n","            output_dimensions=1 + num_components)\n","\n","    def _fit_discrete(self, data):\n","        \"\"\"Fit one hot encoder for discrete column.\n","\n","        Args:\n","            data (pd.DataFrame):\n","                A dataframe containing a column.\n","\n","        Returns:\n","            namedtuple:\n","                A ``ColumnTransformInfo`` object.\n","        \"\"\"\n","        column_name = data.columns[0]\n","        ohe = OneHotEncoder()\n","        ohe.fit(data, column_name)\n","        num_categories = len(ohe.dummies)\n","\n","        return ColumnTransformInfo(\n","            column_name=column_name, column_type='discrete', transform=ohe,\n","            output_info=[SpanInfo(num_categories, 'softmax')],\n","            output_dimensions=num_categories)\n","\n","    def fit(self, raw_data, discrete_columns=()):\n","        \"\"\"Fit the ``DataTransformer``.\n","\n","        Fits a ``ClusterBasedNormalizer`` for continuous columns and a\n","        ``OneHotEncoder`` for discrete columns.\n","\n","        This step also counts the #columns in matrix data and span information.\n","        \"\"\"\n","        self.output_info_list = []\n","        self.output_dimensions = 0\n","        self.dataframe = True\n","\n","        if not isinstance(raw_data, pd.DataFrame):\n","            self.dataframe = False\n","            # work around for RDT issue #328 Fitting with numerical column names fails\n","            discrete_columns = [str(column) for column in discrete_columns]\n","            column_names = [str(num) for num in range(raw_data.shape[1])]\n","            raw_data = pd.DataFrame(raw_data, columns=column_names)\n","\n","        self._column_raw_dtypes = raw_data.infer_objects().dtypes\n","        self._column_transform_info_list = []\n","        for column_name in raw_data.columns:\n","            if column_name in discrete_columns:\n","                column_transform_info = self._fit_discrete(raw_data[[column_name]])\n","            else:\n","                column_transform_info = self._fit_continuous(raw_data[[column_name]])\n","\n","            self.output_info_list.append(column_transform_info.output_info)\n","            self.output_dimensions += column_transform_info.output_dimensions\n","            self._column_transform_info_list.append(column_transform_info)\n","\n","    def _transform_continuous(self, column_transform_info, data):\n","        column_name = data.columns[0]\n","        flattened_column = data[column_name].to_numpy().flatten()\n","        data = data.assign(**{column_name: flattened_column})\n","        gm = column_transform_info.transform\n","        transformed = gm.transform(data)\n","\n","        #  Converts the transformed data to the appropriate output format.\n","        #  The first column (ending in '.normalized') stays the same,\n","        #  but the lable encoded column (ending in '.component') is one hot encoded.\n","        output = np.zeros((len(transformed), column_transform_info.output_dimensions))\n","        output[:, 0] = transformed[f'{column_name}.normalized'].to_numpy()\n","        index = transformed[f'{column_name}.component'].to_numpy().astype(int)\n","        output[np.arange(index.size), index + 1] = 1.0\n","\n","        return output\n","\n","    def _transform_discrete(self, column_transform_info, data):\n","        ohe = column_transform_info.transform\n","        return ohe.transform(data).to_numpy()\n","\n","    def _synchronous_transform(self, raw_data, column_transform_info_list):\n","        \"\"\"Take a Pandas DataFrame and transform columns synchronous.\n","\n","        Outputs a list with Numpy arrays.\n","        \"\"\"\n","        column_data_list = []\n","        for column_transform_info in column_transform_info_list:\n","            column_name = column_transform_info.column_name\n","            data = raw_data[[column_name]]\n","            if column_transform_info.column_type == 'continuous':\n","                column_data_list.append(self._transform_continuous(column_transform_info, data))\n","            else:\n","                column_data_list.append(self._transform_discrete(column_transform_info, data))\n","\n","        return column_data_list\n","\n","    def _parallel_transform(self, raw_data, column_transform_info_list):\n","        \"\"\"Take a Pandas DataFrame and transform columns in parallel.\n","\n","        Outputs a list with Numpy arrays.\n","        \"\"\"\n","        processes = []\n","        for column_transform_info in column_transform_info_list:\n","            column_name = column_transform_info.column_name\n","            data = raw_data[[column_name]]\n","            process = None\n","            if column_transform_info.column_type == 'continuous':\n","                process = delayed(self._transform_continuous)(column_transform_info, data)\n","            else:\n","                process = delayed(self._transform_discrete)(column_transform_info, data)\n","            processes.append(process)\n","\n","        return Parallel(n_jobs=-1)(processes)\n","\n","    def transform(self, raw_data):\n","        \"\"\"Take raw data and output a matrix data.\"\"\"\n","        if not isinstance(raw_data, pd.DataFrame):\n","            column_names = [str(num) for num in range(raw_data.shape[1])]\n","            raw_data = pd.DataFrame(raw_data, columns=column_names)\n","\n","        # Only use parallelization with larger data sizes.\n","        # Otherwise, the transformation will be slower.\n","        if raw_data.shape[0] < 500:\n","            column_data_list = self._synchronous_transform(\n","                raw_data,\n","                self._column_transform_info_list\n","            )\n","        else:\n","            column_data_list = self._parallel_transform(\n","                raw_data,\n","                self._column_transform_info_list\n","            )\n","\n","        return np.concatenate(column_data_list, axis=1).astype(float)\n","\n","    def _inverse_transform_continuous(self, column_transform_info, column_data, sigmas, st):\n","        gm = column_transform_info.transform\n","        data = pd.DataFrame(\n","            column_data[:, :2], columns=list(gm.get_output_sdtypes())).astype(float)\n","        data[data.columns[1]] = np.argmax(column_data[:, 1:], axis=1)\n","        if sigmas is not None:\n","            selected_normalized_value = np.random.normal(data.iloc[:, 0], sigmas[st])\n","            data.iloc[:, 0] = selected_normalized_value\n","\n","        return gm.reverse_transform(data)\n","\n","    def _inverse_transform_discrete(self, column_transform_info, column_data):\n","        ohe = column_transform_info.transform\n","        data = pd.DataFrame(column_data, columns=list(ohe.get_output_sdtypes()))\n","        return ohe.reverse_transform(data)[column_transform_info.column_name]\n","\n","    def inverse_transform(self, data, sigmas=None):\n","        \"\"\"Take matrix data and output raw data.\n","\n","        Output uses the same type as input to the transform function.\n","        Either np array or pd dataframe.\n","        \"\"\"\n","        st = 0\n","        recovered_column_data_list = []\n","        column_names = []\n","        for column_transform_info in self._column_transform_info_list:\n","            dim = column_transform_info.output_dimensions\n","            column_data = data[:, st:st + dim]\n","            if column_transform_info.column_type == 'continuous':\n","                recovered_column_data = self._inverse_transform_continuous(\n","                    column_transform_info, column_data, sigmas, st)\n","            else:\n","                recovered_column_data = self._inverse_transform_discrete(\n","                    column_transform_info, column_data)\n","\n","            recovered_column_data_list.append(recovered_column_data)\n","            column_names.append(column_transform_info.column_name)\n","            st += dim\n","\n","        recovered_data = np.column_stack(recovered_column_data_list)\n","        recovered_data = (pd.DataFrame(recovered_data, columns=column_names)\n","                          .astype(self._column_raw_dtypes))\n","        if not self.dataframe:\n","            recovered_data = recovered_data.to_numpy()\n","\n","        return recovered_data\n","\n","    def convert_column_name_value_to_id(self, column_name, value):\n","        \"\"\"Get the ids of the given `column_name`.\"\"\"\n","        discrete_counter = 0\n","        column_id = 0\n","        for column_transform_info in self._column_transform_info_list:\n","            if column_transform_info.column_name == column_name:\n","                break\n","            if column_transform_info.column_type == 'discrete':\n","                discrete_counter += 1\n","\n","            column_id += 1\n","\n","        else:\n","            raise ValueError(f\"The column_name `{column_name}` doesn't exist in the data.\")\n","\n","        ohe = column_transform_info.transform\n","        data = pd.DataFrame([value], columns=[column_transform_info.column_name])\n","        one_hot = ohe.transform(data).to_numpy()[0]\n","        if sum(one_hot) == 0:\n","            raise ValueError(f\"The value `{value}` doesn't exist in the column `{column_name}`.\")\n","\n","        return {\n","            'discrete_column_id': discrete_counter,\n","            'column_id': column_id,\n","            'value_id': np.argmax(one_hot)\n","        }"],"metadata":{"id":"SxX0MU6gvJht","executionInfo":{"status":"ok","timestamp":1716703010868,"user_tz":-540,"elapsed":1668,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# G, D, Q 네트워크 생성"],"metadata":{"id":"U6R144Karphu"}},{"cell_type":"markdown","source":["# Q네트워크에서 평균이랑 분산 pac 사이즈 고려하지 않고 Batch_size,latent_dim으로 출력하는 판별기 코드"],"metadata":{"id":"cMeP2WAUiYmD"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.nn import Module, Linear, LeakyReLU, Dropout, Sequential\n","\n","class Discriminator(Module):\n","    \"\"\"Discriminator for the CTGAN with D_head and Q_head for InfoGAN.\"\"\"\n","\n","    def __init__(self, input_dim, discriminator_dim, latent_dim, pac=10):\n","        super(Discriminator, self).__init__()\n","        dim = input_dim * pac\n","        self.pac = pac\n","        self.pacdim = dim\n","\n","        # Shared layers between D_head and Q_head\n","        seq = []\n","        for item in list(discriminator_dim):\n","            seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]\n","            dim = item\n","\n","        self.shared = Sequential(*seq)\n","\n","        # D_head for real/fake classification\n","        self.D_head = Linear(dim, 1)\n","\n","        # Q_head for latent code prediction\n","        self.Q_mu = Linear(dim, latent_dim)\n","        self.Q_var = Linear(dim, latent_dim)\n","\n","    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n","        \"\"\"Compute the gradient penalty.\"\"\"\n","        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n","        alpha = alpha.repeat(1, pac, real_data.size(1))\n","        alpha = alpha.view(-1, real_data.size(1))\n","\n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","\n","        disc_interpolates = self(interpolates)[0]\n","\n","        gradients = torch.autograd.grad(\n","            outputs=disc_interpolates, inputs=interpolates,\n","            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n","            create_graph=True, retain_graph=True, only_inputs=True\n","        )[0]\n","\n","        gradients_view = gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n","        gradient_penalty = ((gradients_view) ** 2).mean() * lambda_\n","\n","        return gradient_penalty\n","\n","    def forward(self, input_):\n","        \"\"\"Apply the Discriminator and Q network to the `input_`.\"\"\"\n","        batch_size = input_.size(0)\n","        assert batch_size % self.pac == 0\n","        # Combine input according to pacdim\n","        combined_input = input_.view(batch_size // self.pac, -1)\n","        shared_output = self.shared(combined_input)\n","\n","        # D_head for real/fake classification\n","        disc_output = self.D_head(shared_output)\n","\n","        # Q_head for latent code prediction\n","        mu = self.Q_mu(shared_output)\n","        var = torch.exp(self.Q_var(shared_output))  # Ensure variance is positive\n","\n","        # Repeat mu and var to match batch_size\n","        mu = mu.repeat(self.pac, 1)\n","        var = var.repeat(self.pac, 1)\n","\n","        return disc_output, mu.view(batch_size, -1), var.view(batch_size, -1)\n","\n","# Test with batch size and pac\n","input_dim = 128\n","discriminator_dim = [256, 128]\n","latent_dim = 10\n","pac = 10\n","batch_size = 100\n","\n","discriminator = Discriminator(input_dim, discriminator_dim, latent_dim, pac=pac)\n","\n","# Generate random data for testing\n","real_data = torch.randn(batch_size, input_dim)\n","fake_data = torch.randn(batch_size, input_dim)\n","disc_output, mu, var = discriminator(real_data)\n","print(disc_output.shape, mu.shape, var.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDWuoVg1iXwe","executionInfo":{"status":"ok","timestamp":1716703010869,"user_tz":-540,"elapsed":19,"user":{"displayName":"허주혁","userId":"07084138917445903099"}},"outputId":"f6a04b28-aec3-43fc-bbd9-f918fb45a0cb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 1]) torch.Size([100, 10]) torch.Size([100, 10])\n"]}]},{"cell_type":"markdown","source":["# pac사이즈 고려해서 평균이랑 분산 출력하는 판별기 -> 이렇게 되면 차원이 맞지않음.(Q네트워크는 굳이 pac 사이즈 고려 필요 없음)"],"metadata":{"id":"v0tTVUtDigfn"}},{"cell_type":"code","source":["# import torch\n","# from torch import nn\n","# from torch.nn import Module, Linear, LeakyReLU, Dropout, Sequential\n","\n","# class Discriminator(Module):\n","#     \"\"\"Discriminator for the CTGAN with D_head and Q_head for InfoGAN.\"\"\"\n","\n","#     def __init__(self, input_dim, discriminator_dim, latent_dim, pac=10):\n","#         super(Discriminator, self).__init__()\n","#         dim = input_dim * pac\n","#         self.pac = pac\n","#         self.pacdim = dim\n","\n","#         # Shared layers between D_head and Q_head\n","#         seq = []\n","#         for item in list(discriminator_dim):\n","#             seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]\n","#             dim = item\n","\n","#         self.shared = Sequential(*seq)\n","\n","#         # D_head for real/fake classification\n","#         self.D_head = Linear(dim, 1)\n","\n","#         # Q_head for latent code prediction\n","#         self.Q_mu = Linear(dim, latent_dim)\n","#         self.Q_var = Linear(dim, latent_dim)\n","\n","#     def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n","#         \"\"\"Compute the gradient penalty.\"\"\"\n","#         alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n","#         alpha = alpha.repeat(1, pac, real_data.size(1))\n","#         alpha = alpha.view(-1, real_data.size(1))\n","\n","#         interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","\n","#         disc_interpolates = self(interpolates)[0]\n","\n","#         gradients = torch.autograd.grad(\n","#             outputs=disc_interpolates, inputs=interpolates,\n","#             grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n","#             create_graph=True, retain_graph=True, only_inputs=True\n","#         )[0]\n","\n","#         gradients_view = gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n","#         gradient_penalty = ((gradients_view) ** 2).mean() * lambda_\n","\n","#         return gradient_penalty\n","\n","#     def forward(self, input_):\n","#         \"\"\"Apply the Discriminator and Q network to the `input_`.\"\"\"\n","#         assert input_.size()[0] % self.pac == 0\n","#         shared_output = self.shared(input_.view(-1, self.pacdim))\n","\n","\n","#         # D_head for real/fake classification\n","#         disc_output = self.D_head(shared_output)\n","\n","#         # Q_head for latent code prediction\n","#         mu = self.Q_mu(shared_output)\n","#         var = torch.exp(self.Q_var(shared_output))  # Ensure variance is positive\n","\n","#         return disc_output, mu, var\n","\n","# # Example usage\n","# input_dim = 128\n","# discriminator_dim = [256, 128]\n","# latent_dim = 2\n","# discriminator = Discriminator(input_dim, discriminator_dim, latent_dim)\n","\n","# # Generate random data for testing\n","# real_data = torch.randn(100, input_dim)\n","# print(real_data.shape)\n","# fake_data = torch.randn(100, input_dim)\n","# disc_output, mu, var = discriminator(real_data)\n","# print(disc_output.shape, mu.shape, var.shape)"],"metadata":{"id":"L5y39px5Svrf","executionInfo":{"status":"ok","timestamp":1716703010869,"user_tz":-540,"elapsed":17,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.nn import Module, Linear, BatchNorm1d, ReLU\n","\n","class Residual(Module):\n","    \"\"\"Residual layer for the CTGAN with latent code.\"\"\"\n","\n","    def __init__(self, i, o, latent_dim):\n","        super(Residual, self).__init__()\n","        self.fc = Linear(i + latent_dim, o)\n","        self.bn = BatchNorm1d(o)\n","        self.relu = ReLU()\n","        self.latent_dim = latent_dim\n","\n","    def forward(self, input_, latent_code):\n","        \"\"\"Apply the Residual layer to the `input_` and `latent_code`.\"\"\"\n","        # Concatenate input with latent code\n","        latent_code_expanded = latent_code.expand(input_.size(0), self.latent_dim)\n","        combined_input = torch.cat([input_, latent_code_expanded], dim=1)\n","        out = self.fc(combined_input)\n","        out = self.bn(out)\n","        out = self.relu(out)\n","        return torch.cat([out, input_], dim=1)\n"],"metadata":{"id":"G6uVsq8NSvke","executionInfo":{"status":"ok","timestamp":1716703010869,"user_tz":-540,"elapsed":16,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from torch.nn import Sequential\n","\n","class Generator(Module):\n","    \"\"\"Generator for the CTGAN with latent code.\"\"\"\n","\n","    def __init__(self, embedding_dim, generator_dim, data_dim, latent_dim):\n","        super(Generator, self).__init__()\n","        dim = embedding_dim\n","        self.latent_dim = latent_dim\n","        seq = []\n","        for item in list(generator_dim):\n","            seq += [Residual(dim, item, latent_dim)]\n","            dim += item\n","        seq.append(Linear(dim, data_dim))\n","        self.seq = Sequential(*seq)\n","\n","    def forward(self, input_, latent_code):\n","        \"\"\"Apply the Generator to the `input_` and `latent_code`.\"\"\"\n","        data = input_\n","        for layer in self.seq:\n","            if isinstance(layer, Residual):\n","                data = layer(data, latent_code)\n","            else:\n","                data = layer(data)\n","        return data\n"],"metadata":{"id":"VNJB-6zjSvg5","executionInfo":{"status":"ok","timestamp":1716703010869,"user_tz":-540,"elapsed":16,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def sample(self, n, condition_column=None, condition_value=None):\n","        \"\"\"Sample data similar to the training data.\n","\n","        Choosing a condition_column and condition_value will increase the probability of the\n","        discrete condition_value happening in the condition_column.\n","\n","        Args:\n","            n (int):\n","                Number of rows to sample.\n","            condition_column (string):\n","                Name of a discrete column.\n","            condition_value (string):\n","                Name of the category in the condition_column which we wish to increase the\n","                probability of happening.\n","\n","        Returns:\n","            numpy.ndarray or pandas.DataFrame\n","        \"\"\"\n","        if condition_column is not None and condition_value is not None:\n","            condition_info = self._transformer.convert_column_name_value_to_id(\n","                condition_column, condition_value)\n","            global_condition_vec = self._data_sampler.generate_cond_from_condition_column_info(\n","                condition_info, self._batch_size)\n","        else:\n","            global_condition_vec = None\n","\n","        steps = n // self._batch_size + 1\n","        data = []\n","        for i in range(steps):\n","            # 노이즈 생성\n","            mean = torch.zeros(self._batch_size, self._embedding_dim)\n","            std = mean + 1\n","            fakez = torch.normal(mean=mean, std=std).to(self._device)\n","\n","            if global_condition_vec is not None:\n","                condvec = global_condition_vec.copy()\n","            else:\n","                condvec = self._data_sampler.sample_original_condvec(self._batch_size)\n","\n","            if condvec is None:\n","                pass\n","            else:\n","                c1 = condvec\n","                c1 = torch.from_numpy(c1).to(self._device)\n","                fakez = torch.cat([fakez, c1], dim=1)\n","\n","            fake = self._generator(fakez)\n","            fakeact = self._apply_activate(fake)\n","            data.append(fakeact.detach().cpu().numpy())\n","\n","        data = np.concatenate(data, axis=0)\n","        data = data[:n]\n","\n","        return self._transformer.inverse_transform(data)"],"metadata":{"id":"_gQeDxZjSvdn","executionInfo":{"status":"ok","timestamp":1716703010870,"user_tz":-540,"elapsed":17,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","class MutualInformationLoss(torch.nn.Module):\n","    def __init__(self):\n","        super(MutualInformationLoss, self).__init__()\n","\n","    def forward(self, x, mu, var):\n","        logli = -0.5 * (var.mul(2 * np.pi) + 1e-6).log() - (x - mu).pow(2).div(var.mul(2.0) + 1e-6)\n","        nll = -(logli.sum(1).mean())\n","        return nll\n"],"metadata":{"id":"LRBtPXKHiISi","executionInfo":{"status":"ok","timestamp":1716703010870,"user_tz":-540,"elapsed":16,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# # CTGAN\n","# from torch import nn, optim\n","# from tqdm import tqdm\n","# from torch.nn import functional\n","# import warnings\n","\n","# import numpy as np\n","# import pandas as pd\n","# import torch\n","# from torch import optim\n","# from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential, functional\n","# from tqdm import tqdm\n","\n","\n","# class CTGAN(BaseSynthesizer):\n","#     \"\"\"Conditional Table GAN Synthesizer.\n","\n","#     This is the core class of the CTGAN project, where the different components\n","#     are orchestrated together.\n","#     For more details about the process, please check the [Modeling Tabular data using\n","#     Conditional GAN](https://arxiv.org/abs/1907.00503) paper.\n","\n","#     Args:\n","#         embedding_dim (int):\n","#             Size of the random sample passed to the Generator. Defaults to 128.\n","#         generator_dim (tuple or list of ints):\n","#             Size of the output samples for each one of the Residuals. A Residual Layer\n","#             will be created for each one of the values provided. Defaults to (256, 256).\n","#         discriminator_dim (tuple or list of ints):\n","#             Size of the output samples for each one of the Discriminator Layers. A Linear Layer\n","#             will be created for each one of the values provided. Defaults to (256, 256).\n","#         generator_lr (float):\n","#             Learning rate for the generator. Defaults to 2e-4.\n","#         generator_decay (float):\n","#             Generator weight decay for the Adam Optimizer. Defaults to 1e-6.\n","#         discriminator_lr (float):\n","#             Learning rate for the discriminator. Defaults to 2e-4.\n","#         discriminator_decay (float):\n","#             Discriminator weight decay for the Adam Optimizer. Defaults to 1e-6.\n","#         batch_size (int):\n","#             Number of data samples to process in each step.\n","#         discriminator_steps (int):\n","#             Number of discriminator updates to do for each generator update.\n","#             From the WGAN paper: https://arxiv.org/abs/1701.07875. WGAN paper\n","#             default is 5. Default used is 1 to match original CTGAN implementation.\n","#         log_frequency (boolean):\n","#             Whether to use log frequency of categorical levels in conditional\n","#             sampling. Defaults to ``True``.\n","#         verbose (boolean):\n","#             Whether to have print statements for progress results. Defaults to ``False``.\n","#         epochs (int):\n","#             Number of training epochs. Defaults to 300.\n","#         pac (int):\n","#             Number of samples to group together when applying the discriminator.\n","#             Defaults to 10.\n","#         cuda (bool):\n","#             Whether to attempt to use cuda for GPU computation.\n","#             If this is False or CUDA is not available, CPU will be used.\n","#             Defaults to ``True``.\n","#     \"\"\"\n","\n","#     def __init__(self, embedding_dim=128, generator_dim=(256, 256), discriminator_dim=(256, 256),\n","#                  latent_dim=1, generator_lr=2e-4, generator_decay=1e-6, discriminator_lr=2e-4,\n","#                  discriminator_decay=1e-6, batch_size=500, discriminator_steps=1,\n","#                  log_frequency=True, verbose=False, epochs=300, pac=10, cuda=True):\n","\n","\n","#         assert batch_size % 2 == 0\n","\n","#         self._embedding_dim = embedding_dim\n","#         self._generator_dim = generator_dim\n","#         self._discriminator_dim = discriminator_dim\n","#         self._latent_dim = latent_dim  # latent_dim 추가\n","\n","#         self._generator_lr = generator_lr\n","#         self._generator_decay = generator_decay\n","#         self._discriminator_lr = discriminator_lr\n","#         self._discriminator_decay = discriminator_decay\n","\n","#         self._batch_size = batch_size\n","#         self._discriminator_steps = discriminator_steps\n","#         self._log_frequency = log_frequency\n","#         self._verbose = verbose\n","#         self._epochs = epochs\n","#         self.pac = pac\n","\n","#         if not cuda or not torch.cuda.is_available():\n","#             device = 'cpu'\n","#         elif isinstance(cuda, str):\n","#             device = cuda\n","#         else:\n","#             device = 'cuda'\n","\n","#         self._device = torch.device(device)\n","\n","#         self._transformer = None\n","#         self._data_sampler = None\n","#         self._generator = None\n","#         self.loss_values = None\n","\n","\n","#     @staticmethod\n","#     def _gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):\n","#         \"\"\"Deals with the instability of the gumbel_softmax for older versions of torch.\n","\n","#         For more details about the issue:\n","#         https://drive.google.com/file/d/1AA5wPfZ1kquaRtVruCd6BiYZGcDeNxyP/view?usp=sharing\n","\n","#         Args:\n","#             logits […, num_features]:\n","#                 Unnormalized log probabilities\n","#             tau:\n","#                 Non-negative scalar temperature\n","#             hard (bool):\n","#                 If True, the returned samples will be discretized as one-hot vectors,\n","#                 but will be differentiated as if it is the soft sample in autograd\n","#             dim (int):\n","#                 A dimension along which softmax will be computed. Default: -1.\n","\n","#         Returns:\n","#             Sampled tensor of same shape as logits from the Gumbel-Softmax distribution.\n","#         \"\"\"\n","#         for _ in range(10):\n","#             transformed = functional.gumbel_softmax(logits, tau=tau, hard=hard, eps=eps, dim=dim)\n","#             if not torch.isnan(transformed).any():\n","#                 return transformed\n","\n","#         raise ValueError('gumbel_softmax returning NaN.')\n","\n","\n","#     def _apply_activate(self, data):\n","#       \"\"\"Apply proper activation function to the output of the generator.\"\"\"\n","#       data_t = []\n","#       st = 0\n","#       for column_info in self._transformer.output_info_list:\n","#           for span_info in column_info:\n","#               if span_info.activation_fn == 'tanh':\n","#                   ed = st + span_info.dim\n","#                   data_t.append(torch.tanh(data[:, st:ed]))\n","#                   st = ed\n","#               elif span_info.activation_fn == 'softmax':\n","#                   ed = st + span_info.dim\n","#                   transformed = self._gumbel_softmax(data[:, st:ed], tau=0.2)\n","#                   data_t.append(transformed)\n","#                   st = ed\n","#               else:\n","#                   raise ValueError(f'Unexpected activation function {span_info.activation_fn}.')\n","\n","#       return torch.cat(data_t, dim=1)\n","\n","#     def _cond_loss(self, data, c, m):\n","#       \"\"\"Compute the cross entropy loss on the fixed discrete column.\"\"\"\n","#       loss = []\n","#       st = 0\n","#       st_c = 0\n","#       for column_info in self._transformer.output_info_list:\n","#           for span_info in column_info:\n","#               if len(column_info) != 1 or span_info.activation_fn != 'softmax':\n","#                   # not discrete column\n","#                   st += span_info.dim\n","#               else:\n","#                   ed = st + span_info.dim\n","#                   ed_c = st_c + span_info.dim\n","#                   tmp = functional.cross_entropy(\n","#                       data[:, st:ed],\n","#                       torch.argmax(c[:, st_c:ed_c], dim=1),\n","#                       reduction='none'\n","#                   )\n","#                   loss.append(tmp)\n","#                   st = ed\n","#                   st_c = ed_c\n","\n","#       loss = torch.stack(loss, dim=1)  # noqa: PD013\n","\n","#       return (loss * m).sum() / data.size()[0]\n","\n","\n","#     # 이산형 열이 존재하는지\n","#     def _validate_discrete_columns(self, train_data, discrete_columns):\n","#       \"\"\"Check whether ``discrete_columns`` exists in ``train_data``.\n","\n","#       Args:\n","#           train_data (numpy.ndarray or pandas.DataFrame):\n","#               Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.\n","#           discrete_columns (list-like):\n","#               List of discrete columns to be used to generate the Conditional\n","#               Vector. If ``train_data`` is a Numpy array, this list should\n","#               contain the integer indices of the columns. Otherwise, if it is\n","#               a ``pandas.DataFrame``, this list should contain the column names.\n","#       \"\"\"\n","#       if isinstance(train_data, pd.DataFrame):\n","#           invalid_columns = set(discrete_columns) - set(train_data.columns)\n","#       elif isinstance(train_data, np.ndarray):\n","#           invalid_columns = []\n","#           for column in discrete_columns:\n","#               if column < 0 or column >= train_data.shape[1]:\n","#                   invalid_columns.append(column)\n","#       else:\n","#           raise TypeError('``train_data`` should be either pd.DataFrame or np.array.')\n","\n","#       if invalid_columns:\n","#           raise ValueError(f'Invalid columns found: {invalid_columns}')\n","\n","#     @random_state\n","#     def fit(self, train_data, discrete_columns=(), epochs=None):\n","#       \"\"\"Fit the CTGAN Synthesizer models to the training data.\n","#       # CTGAN 모델 학습 (latent code 추가)\n","\n","#       Args:\n","#           train_data (numpy.ndarray or pandas.DataFrame):\n","#               Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.\n","#           discrete_columns (list-like):\n","#               List of discrete columns to be used to generate the Conditional\n","#               Vector. If ``train_data`` is a Numpy array, this list should\n","#               contain the integer indices of the columns. Otherwise, if it is\n","#               a ``pandas.DataFrame``, this list should contain the column names.\n","#       \"\"\"\n","#       self._validate_discrete_columns(train_data, discrete_columns)\n","\n","#       if epochs is None:\n","#           epochs = self._epochs\n","#       else:\n","#           warnings.warn(\n","#               ('`epochs` argument in `fit` method has been deprecated and will be removed '\n","#                 'in a future version. Please pass `epochs` to the constructor instead'),\n","#               DeprecationWarning\n","#           )\n","\n","#       self._transformer = DataTransformer()\n","#       self._transformer.fit(train_data, discrete_columns)\n","\n","#       train_data = self._transformer.transform(train_data)\n","\n","#       self._data_sampler = DataSampler(\n","#           train_data,\n","#           self._transformer.output_info_list,\n","#           self._log_frequency)\n","\n","#       data_dim = self._transformer.output_dimensions\n","\n","#       self._generator = Generator(\n","#           self._embedding_dim + self._data_sampler.dim_cond_vec(),\n","#           self._generator_dim,\n","#           data_dim,\n","#           self._latent_dim  # latent_dim 추가\n","#       ).to(self._device)\n","\n","#       discriminator = Discriminator(\n","#           data_dim + self._data_sampler.dim_cond_vec(),\n","#           self._discriminator_dim,\n","#           latent_dim=self._latent_dim,  # latent_dim 추가\n","#           pac=self.pac\n","#       ).to(self._device)\n","\n","#       optimizerG = optim.Adam(\n","#           self._generator.parameters(), lr=self._generator_lr, betas=(0.5, 0.9),\n","#           weight_decay=self._generator_decay\n","#       )\n","\n","#       optimizerD = optim.Adam(\n","#           discriminator.parameters(), lr=self._discriminator_lr,\n","#           betas=(0.5, 0.9), weight_decay=self._discriminator_decay\n","#       )\n","\n","#       mean = torch.zeros(self._batch_size, self._embedding_dim, device=self._device)\n","#       std = torch.ones(self._batch_size, self._embedding_dim, device=self._device)\n","\n","#       self.loss_values = pd.DataFrame(columns=['Epoch', 'Generator Loss', 'Discriminator Loss'])\n","\n","#       epoch_iterator = tqdm(range(epochs), disable=(not self._verbose))\n","#       if self._verbose:\n","#           description = 'Gen. ({gen:.2f}) | Discrim. ({dis:.2f})'\n","#           epoch_iterator.set_description(description.format(gen=0, dis=0))\n","\n","#       steps_per_epoch = max(len(train_data) // self._batch_size, 1)\n","#       for i in epoch_iterator:\n","#           for id_ in range(steps_per_epoch):\n","\n","#               for n in range(self._discriminator_steps):\n","#                   fakez = torch.normal(mean=mean, std=std)\n","#                   # latent_code = torch.normal(mean=torch.zeros(self._batch_size, self._latent_dim), std=torch.ones(self._batch_size, self._latent_dim)).to(self._device)  # latent_code 추가\n","#                   latent_code = torch.rand(self._batch_size, self._latent_dim, device=self._device) * 2 - 1  # uniform 분포에서 latent code 생성\n","\n","#                   condvec = self._data_sampler.sample_condvec(self._batch_size)\n","#                   if condvec is None:\n","#                       c1, m1, col, opt = None, None, None, None\n","#                       real = self._data_sampler.sample_data(\n","#                           train_data, self._batch_size, col, opt)\n","#                   else:\n","#                       c1, m1, col, opt = condvec\n","#                       c1 = torch.from_numpy(c1).to(self._device)\n","#                       m1 = torch.from_numpy(m1).to(self._device)\n","#                       fakez = torch.cat([fakez, c1], dim=1)\n","\n","#                       perm = np.arange(self._batch_size)\n","#                       np.random.shuffle(perm)\n","#                       real = self._data_sampler.sample_data(\n","#                           train_data, self._batch_size, col[perm], opt[perm])\n","#                       c2 = c1[perm]\n","#                   # conditional vector와 noise에다가 latent_code 추가\n","#                   fake = self._generator(fakez, latent_code)  # latent_code 추가\n","#                   fakeact = self._apply_activate(fake)\n","\n","#                   real = torch.from_numpy(real.astype('float32')).to(self._device)\n","\n","#                   if c1 is not None:\n","#                       fake_cat = torch.cat([fakeact, c1], dim=1)\n","#                       real_cat = torch.cat([real, c2], dim=1)\n","#                   else:\n","#                       real_cat = real\n","#                       fake_cat = fakeact\n","\n","#                   y_fake = discriminator(fake_cat)\n","#                   y_real = discriminator(real_cat)\n","\n","#                   pen = discriminator.calc_gradient_penalty(\n","#                       real_cat, fake_cat, self._device, self.pac)\n","#                   loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n","\n","#                   optimizerD.zero_grad(set_to_none=False)\n","#                   pen.backward(retain_graph=True)\n","#                   loss_d.backward()\n","#                   optimizerD.step()\n","\n","#               fakez = torch.normal(mean=mean, std=std)\n","#               # latent_code = torch.normal(mean=torch.zeros(self._batch_size, self._latent_dim), std=torch.ones(self._batch_size, self._latent_dim)).to(self._device)  # latent_code 추가\n","#               latent_code = torch.rand(self._batch_size, self._latent_dim, device=self._device) * 2 - 1  # uniform 분포에서 latent_code 생성\n","#               condvec = self._data_sampler.sample_condvec(self._batch_size)\n","\n","#               if condvec is None:\n","#                   c1, m1, col, opt = None, None, None, None\n","#               else:\n","#                   c1, m1, col, opt = condvec\n","#                   c1 = torch.from_numpy(c1).to(self._device)\n","#                   m1 = torch.from_numpy(m1).to(self._device)\n","#                   fakez = torch.cat([fakez, c1], dim=1)\n","\n","#               fake = self._generator(fakez, latent_code)  # latent_code 추가\n","#               fakeact = self._apply_activate(fake)\n","\n","#               if c1 is not None:\n","#                   y_fake = discriminator(torch.cat([fakeact, c1], dim=1))\n","#               else:\n","#                   y_fake = discriminator(fakeact)\n","\n","#               if condvec is None:\n","#                   cross_entropy = 0\n","#               else:\n","#                   cross_entropy = self._cond_loss(fake, c1, m1)\n","\n","#               mu, var = self._q_network(fake)  # Q 네트워크로부터 mu와 var 추출\n","#               mi_loss = self.mutual_information_loss(latent_code, mu, var)  # mutual information loss 계산\n","\n","#               loss_g = -torch.mean(y_fake) + cross_entropy + mi_loss  # mutual information loss를 Generator 손실에 추가\n","\n","#               optimizerG.zero_grad(set_to_none=False)\n","#               loss_g.backward()\n","#               optimizerG.step()\n","\n","#           generator_loss = loss_g.detach().cpu().item()\n","#           discriminator_loss = loss_d.detach().cpu().item()\n","\n","#           epoch_loss_df = pd.DataFrame({\n","#               'Epoch': [i],\n","#               'Generator Loss': [generator_loss],\n","#               'Discriminator Loss': [discriminator_loss]\n","#           })\n","#           if not self.loss_values.empty:\n","#               self.loss_values = pd.concat(\n","#                   [self.loss_values, epoch_loss_df]\n","#               ).reset_index(drop=True)\n","#           else:\n","#               self.loss_values = epoch_loss_df\n","\n","#           if self._verbose:\n","#               epoch_iterator.set_description(\n","#                   description.format(gen=generator_loss, dis=discriminator_loss)\n","#               )\n","\n","\n","\n","#     def sample(self, n, condition_column=None, condition_value=None):\n","#         \"\"\"Sample data similar to the training data.\n","\n","#         Choosing a condition_column and condition_value will increase the probability of the\n","#         discrete condition_value happening in the condition_column.\n","\n","#         Args:\n","#             n (int):\n","#                 Number of rows to sample.\n","#             condition_column (string):\n","#                 Name of a discrete column.\n","#             condition_value (string):\n","#                 Name of the category in the condition_column which we wish to increase the\n","#                 probability of happening.\n","\n","#         Returns:\n","#             numpy.ndarray or pandas.DataFrame\n","#         \"\"\"\n","#         if condition_column is not None and condition_value is not None:\n","#             condition_info = self._transformer.convert_column_name_value_to_id(\n","#                 condition_column, condition_value)\n","#             global_condition_vec = self._data_sampler.generate_cond_from_condition_column_info(\n","#                 condition_info, self._batch_size)\n","#         else:\n","#             global_condition_vec = None\n","\n","#         steps = n // self._batch_size + 1\n","#         data = []\n","#         for i in range(steps):\n","#             mean = torch.zeros(self._batch_size, self._embedding_dim)\n","#             std = mean + 1\n","#             fakez = torch.normal(mean=mean, std=std).to(self._device)\n","#             latent_code = torch.rand(self._batch_size, self._latent_dim, device=self._device) * 2 - 1  # uniform 분포에서 latent code 생성\n","\n","#             if global_condition_vec is not None:\n","#                 condvec = global_condition_vec.copy()\n","#             else:\n","#                 condvec = self._data_sampler.sample_original_condvec(self._batch_size)\n","\n","#             if condvec is None:\n","#                 pass\n","#             else:\n","#                 c1 = condvec\n","#                 c1 = torch.from_numpy(c1).to(self._device)\n","#                 fakez = torch.cat([fakez, c1], dim=1)\n","\n","#             fake = self._generator(fakez, latent_code) # latent code 추가\n","#             fakeact = self._apply_activate(fake)\n","#             data.append(fakeact.detach().cpu().numpy())\n","\n","#         data = np.concatenate(data, axis=0)\n","#         data = data[:n]\n","\n","#         return self._transformer.inverse_transform(data)\n","\n","#     def set_device(self, device):\n","#       \"\"\"Set the `device` to be used ('GPU' or 'CPU).\"\"\"\n","#       self._device = device\n","#       if self._generator is not None:\n","#           self._generator.to(self._device)\n","#       if self._discriminator is not None:\n","#           self._discriminator.to(self._device)\n","#       if self._q_network is not None:\n","#           self._q_network.to(self._device)\n"],"metadata":{"id":"fLX7FCCr1gZ0","executionInfo":{"status":"ok","timestamp":1716703010870,"user_tz":-540,"elapsed":16,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# CTGAN\n","from torch import nn, optim\n","from tqdm import tqdm\n","from torch.nn import functional\n","import warnings\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import optim\n","from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential, functional\n","from tqdm import tqdm\n","\n","\n","class CTGAN(BaseSynthesizer):\n","    \"\"\"Conditional Table GAN Synthesizer.\n","\n","    This is the core class of the CTGAN project, where the different components\n","    are orchestrated together.\n","    For more details about the process, please check the [Modeling Tabular data using\n","    Conditional GAN](https://arxiv.org/abs/1907.00503) paper.\n","\n","    Args:\n","        embedding_dim (int):\n","            Size of the random sample passed to the Generator. Defaults to 128.\n","        generator_dim (tuple or list of ints):\n","            Size of the output samples for each one of the Residuals. A Residual Layer\n","            will be created for each one of the values provided. Defaults to (256, 256).\n","        discriminator_dim (tuple or list of ints):\n","            Size of the output samples for each one of the Discriminator Layers. A Linear Layer\n","            will be created for each one of the values provided. Defaults to (256, 256).\n","        generator_lr (float):\n","            Learning rate for the generator. Defaults to 2e-4.\n","        generator_decay (float):\n","            Generator weight decay for the Adam Optimizer. Defaults to 1e-6.\n","        discriminator_lr (float):\n","            Learning rate for the discriminator. Defaults to 2e-4.\n","        discriminator_decay (float):\n","            Discriminator weight decay for the Adam Optimizer. Defaults to 1e-6.\n","        batch_size (int):\n","            Number of data samples to process in each step.\n","        discriminator_steps (int):\n","            Number of discriminator updates to do for each generator update.\n","            From the WGAN paper: https://arxiv.org/abs/1701.07875. WGAN paper\n","            default is 5. Default used is 1 to match original CTGAN implementation.\n","        log_frequency (boolean):\n","            Whether to use log frequency of categorical levels in conditional\n","            sampling. Defaults to ``True``.\n","        verbose (boolean):\n","            Whether to have print statements for progress results. Defaults to ``False``.\n","        epochs (int):\n","            Number of training epochs. Defaults to 300.\n","        pac (int):\n","            Number of samples to group together when applying the discriminator.\n","            Defaults to 10.\n","        cuda (bool):\n","            Whether to attempt to use cuda for GPU computation.\n","            If this is False or CUDA is not available, CPU will be used.\n","            Defaults to ``True``.\n","    \"\"\"\n","\n","    def __init__(self, embedding_dim=128, generator_dim=(256, 256), discriminator_dim=(256, 256),\n","                 latent_dim=1, generator_lr=2e-4, generator_decay=1e-6, discriminator_lr=2e-4,\n","                 discriminator_decay=1e-6, batch_size=500, discriminator_steps=1,\n","                 log_frequency=True, verbose=False, epochs=300, pac=10, cuda=True):\n","\n","\n","        assert batch_size % 2 == 0\n","\n","        self._embedding_dim = embedding_dim\n","        self._generator_dim = generator_dim\n","        self._discriminator_dim = discriminator_dim\n","        self._latent_dim = latent_dim  # latent_dim 추가\n","\n","        self._generator_lr = generator_lr\n","        self._generator_decay = generator_decay\n","        self._discriminator_lr = discriminator_lr\n","        self._discriminator_decay = discriminator_decay\n","\n","        self._batch_size = batch_size\n","        self._discriminator_steps = discriminator_steps\n","        self._log_frequency = log_frequency\n","        self._verbose = verbose\n","        self._epochs = epochs\n","        self.pac = pac\n","\n","        if not cuda or not torch.cuda.is_available():\n","            device = 'cpu'\n","        elif isinstance(cuda, str):\n","            device = cuda\n","        else:\n","            device = 'cuda'\n","\n","        self._device = torch.device(device)\n","\n","        self._transformer = None\n","        self._data_sampler = None\n","        self._generator = None\n","        self.loss_values = None\n","        self._discriminator = None  # Discriminator 초기화 변수 추가\n","        self._q_network = None  # Q 네트워크 초기화 변수 추가\n","        self.mutual_information_loss = MutualInformationLoss()  # MutualInformationLoss 인스턴스 생성\n","\n","    @staticmethod\n","    def _gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):\n","        \"\"\"Deals with the instability of the gumbel_softmax for older versions of torch.\n","\n","        For more details about the issue:\n","        https://drive.google.com/file/d/1AA5wPfZ1kquaRtVruCd6BiYZGcDeNxyP/view?usp=sharing\n","\n","        Args:\n","            logits […, num_features]:\n","                Unnormalized log probabilities\n","            tau:\n","                Non-negative scalar temperature\n","            hard (bool):\n","                If True, the returned samples will be discretized as one-hot vectors,\n","                but will be differentiated as if it is the soft sample in autograd\n","            dim (int):\n","                A dimension along which softmax will be computed. Default: -1.\n","\n","        Returns:\n","            Sampled tensor of same shape as logits from the Gumbel-Softmax distribution.\n","        \"\"\"\n","        for _ in range(10):\n","            transformed = functional.gumbel_softmax(logits, tau=tau, hard=hard, eps=eps, dim=dim)\n","            if not torch.isnan(transformed).any():\n","                return transformed\n","\n","        raise ValueError('gumbel_softmax returning NaN.')\n","\n","    def _apply_activate(self, data):\n","        \"\"\"Apply proper activation function to the output of the generator.\"\"\"\n","        data_t = []\n","        st = 0\n","        for column_info in self._transformer.output_info_list:\n","            for span_info in column_info:\n","                if span_info.activation_fn == 'tanh':\n","                    ed = st + span_info.dim\n","                    data_t.append(torch.tanh(data[:, st:ed]))\n","                    st = ed\n","                elif span_info.activation_fn == 'softmax':\n","                    ed = st + span_info.dim\n","                    transformed = self._gumbel_softmax(data[:, st:ed], tau=0.2)\n","                    data_t.append(transformed)\n","                    st = ed\n","                else:\n","                    raise ValueError(f'Unexpected activation function {span_info.activation_fn}.')\n","\n","        return torch.cat(data_t, dim=1)\n","\n","    def _cond_loss(self, data, c, m):\n","        \"\"\"Compute the cross entropy loss on the fixed discrete column.\"\"\"\n","        loss = []\n","        st = 0\n","        st_c = 0\n","        for column_info in self._transformer.output_info_list:\n","            for span_info in column_info:\n","                if len(column_info) != 1 or span_info.activation_fn != 'softmax':\n","                    # not discrete column\n","                    st += span_info.dim\n","                else:\n","                    ed = st + span_info.dim\n","                    ed_c = st_c + span_info.dim\n","                    tmp = functional.cross_entropy(\n","                        data[:, st:ed],\n","                        torch.argmax(c[:, st_c:ed_c], dim=1),\n","                        reduction='none'\n","                    )\n","                    loss.append(tmp)\n","                    st = ed\n","                    st_c = ed_c\n","\n","        loss = torch.stack(loss, dim=1)  # noqa: PD013\n","\n","        return (loss * m).sum() / data.size()[0]\n","\n","    def _validate_discrete_columns(self, train_data, discrete_columns):\n","        \"\"\"Check whether ``discrete_columns`` exists in ``train_data``.\n","\n","        Args:\n","            train_data (numpy.ndarray or pandas.DataFrame):\n","                Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.\n","            discrete_columns (list-like):\n","                List of discrete columns to be used to generate the Conditional\n","                Vector. If ``train_data`` is a Numpy array, this list should\n","                contain the integer indices of the columns. Otherwise, if it is\n","                a ``pandas.DataFrame``, this list should contain the column names.\n","        \"\"\"\n","        if isinstance(train_data, pd.DataFrame):\n","            invalid_columns = set(discrete_columns) - set(train_data.columns)\n","        elif isinstance(train_data, np.ndarray):\n","            invalid_columns = []\n","            for column in discrete_columns:\n","                if column < 0 or column >= train_data.shape[1]:\n","                    invalid_columns.append(column)\n","        else:\n","            raise TypeError('``train_data`` should be either pd.DataFrame or np.array.')\n","\n","        if invalid_columns:\n","            raise ValueError(f'Invalid columns found: {invalid_columns}')\n","\n","    @random_state\n","    def fit(self, train_data, discrete_columns=(), epochs=None):\n","        \"\"\"Fit the CTGAN Synthesizer models to the training data.\n","        # CTGAN 모델 학습 (latent code 추가)\n","\n","        Args:\n","            train_data (numpy.ndarray or pandas.DataFrame):\n","                Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.\n","            discrete_columns (list-like):\n","                List of discrete columns to be used to generate the Conditional\n","                Vector. If ``train_data`` is a Numpy array, this list should\n","                contain the integer indices of the columns. Otherwise, if it is\n","                a ``pandas.DataFrame``, this list should contain the column names.\n","        \"\"\"\n","        self._validate_discrete_columns(train_data, discrete_columns)\n","\n","        if epochs is None:\n","            epochs = self._epochs\n","        else:\n","            warnings.warn(\n","                ('`epochs` argument in `fit` method has been deprecated and will be removed '\n","                 'in a future version. Please pass `epochs` to the constructor instead'),\n","                DeprecationWarning\n","            )\n","\n","        self._transformer = DataTransformer()\n","        self._transformer.fit(train_data, discrete_columns)\n","\n","        train_data = self._transformer.transform(train_data)\n","\n","        self._data_sampler = DataSampler(\n","            train_data,\n","            self._transformer.output_info_list,\n","            self._log_frequency)\n","\n","        data_dim = self._transformer.output_dimensions\n","\n","        self._generator = Generator(\n","            self._embedding_dim + self._data_sampler.dim_cond_vec(),\n","            self._generator_dim,\n","            data_dim,\n","            self._latent_dim  # latent_dim 추가\n","        ).to(self._device)\n","\n","        self._discriminator = Discriminator(\n","            data_dim + self._data_sampler.dim_cond_vec(),\n","            self._discriminator_dim,\n","            latent_dim=self._latent_dim,  # latent_dim 추가\n","            pac=self.pac\n","        ).to(self._device)\n","\n","        optimizerG = optim.Adam(\n","            self._generator.parameters(), lr=self._generator_lr, betas=(0.5, 0.9),\n","            weight_decay=self._generator_decay\n","        )\n","\n","        optimizerD = optim.Adam(\n","            self._discriminator.parameters(), lr=self._discriminator_lr,\n","            betas=(0.5, 0.9), weight_decay=self._discriminator_decay\n","        )\n","\n","        mean = torch.zeros(self._batch_size, self._embedding_dim, device=self._device)\n","        std = torch.ones(self._batch_size, self._embedding_dim, device=self._device)\n","\n","        self.loss_values = pd.DataFrame(columns=['Epoch', 'Generator Loss', 'Discriminator Loss'])\n","\n","        epoch_iterator = tqdm(range(epochs), disable=(not self._verbose))\n","        if self._verbose:\n","            description = 'Gen. ({gen:.2f}) | Discrim. ({dis:.2f})'\n","            epoch_iterator.set_description(description.format(gen=0, dis=0))\n","\n","        steps_per_epoch = max(len(train_data) // self._batch_size, 1)\n","        for i in epoch_iterator:\n","            for id_ in range(steps_per_epoch):\n","\n","                for n in range(self._discriminator_steps):\n","                    fakez = torch.normal(mean=mean, std=std)\n","                    latent_code = torch.rand(self._batch_size, self._latent_dim, device=self._device) * 2 - 1  # uniform 분포에서 latent code 생성\n","\n","                    condvec = self._data_sampler.sample_condvec(self._batch_size)\n","                    if condvec is None:\n","                        c1, m1, col, opt = None, None, None, None\n","                        real = self._data_sampler.sample_data(\n","                            train_data, self._batch_size, col, opt)\n","                    else:\n","                        c1, m1, col, opt = condvec\n","                        c1 = torch.from_numpy(c1).to(self._device)\n","                        m1 = torch.from_numpy(m1).to(self._device)\n","                        fakez = torch.cat([fakez, c1], dim=1)\n","\n","                        perm = np.arange(self._batch_size)\n","                        np.random.shuffle(perm)\n","                        real = self._data_sampler.sample_data(\n","                            train_data, self._batch_size, col[perm], opt[perm])\n","                        c2 = c1[perm]\n","                    # conditional vector와 noise에다가 latent_code 추가\n","                    fake = self._generator(fakez, latent_code)  # latent_code 추가\n","                    fakeact = self._apply_activate(fake)\n","\n","                    real = torch.from_numpy(real.astype('float32')).to(self._device)\n","\n","                    if c1 is not None:\n","                        fake_cat = torch.cat([fakeact, c1], dim=1)\n","                        real_cat = torch.cat([real, c2], dim=1)\n","                    else:\n","                        real_cat = real\n","                        fake_cat = fakeact\n","\n","                    y_fake, _, _ = self._discriminator(fake_cat)\n","                    y_real, _, _ = self._discriminator(real_cat)\n","\n","                    pen = self._discriminator.calc_gradient_penalty(\n","                        real_cat, fake_cat, self._device, self.pac)\n","                    loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n","\n","                    optimizerD.zero_grad(set_to_none=False)\n","                    pen.backward(retain_graph=True)\n","                    loss_d.backward()\n","                    optimizerD.step()\n","\n","                fakez = torch.normal(mean=mean, std=std)\n","                latent_code = torch.rand(self._batch_size, self._latent_dim, device=self._device) * 2 - 1  # uniform 분포에서 latent code 생성\n","                condvec = self._data_sampler.sample_condvec(self._batch_size)\n","\n","                if condvec is None:\n","                    c1, m1, col, opt = None, None, None, None\n","                else:\n","                    c1, m1, col, opt = condvec\n","                    c1 = torch.from_numpy(c1).to(self._device)\n","                    m1 = torch.from_numpy(m1).to(self._device)\n","                    fakez = torch.cat([fakez, c1], dim=1)\n","\n","                fake = self._generator(fakez, latent_code)  # latent_code 추가\n","                fakeact = self._apply_activate(fake)\n","\n","                if c1 is not None:\n","                    y_fake, mu, var = self._discriminator(torch.cat([fakeact, c1], dim=1))\n","                else:\n","                    y_fake, mu, var = self._discriminator(fakeact)\n","\n","                if condvec is None:\n","                    cross_entropy = 0\n","                else:\n","                    cross_entropy = self._cond_loss(fake, c1, m1)\n","\n","                # 여기서 latent_code만 있는게 아니라 fake 데이터에서 연속형 변수만큼만 뽑아야 함\n","                mi_loss = self.mutual_information_loss(latent_code, mu, var)  # mutual information loss 계산\n","\n","                loss_g = -torch.mean(y_fake) + cross_entropy + mi_loss  # mutual information loss를 Generator 손실에 추가\n","\n","                optimizerG.zero_grad(set_to_none=False)\n","                loss_g.backward()\n","                optimizerG.step()\n","\n","            generator_loss = loss_g.detach().cpu().item()\n","            discriminator_loss = loss_d.detach().cpu().item()\n","\n","            epoch_loss_df = pd.DataFrame({\n","                'Epoch': [i],\n","                'Generator Loss': [generator_loss],\n","                'Discriminator Loss': [discriminator_loss]\n","            })\n","            if not self.loss_values.empty:\n","                self.loss_values = pd.concat(\n","                    [self.loss_values, epoch_loss_df]\n","                ).reset_index(drop=True)\n","            else:\n","                self.loss_values = epoch_loss_df\n","\n","            if self._verbose:\n","                epoch_iterator.set_description(\n","                    description.format(gen=generator_loss, dis=discriminator_loss)\n","                )\n","\n","    def sample(self, n, condition_column=None, condition_value=None):\n","        \"\"\"Sample data similar to the training data.\n","\n","        Choosing a condition_column and condition_value will increase the probability of the\n","        discrete condition_value happening in the condition_column.\n","\n","        Args:\n","            n (int):\n","                Number of rows to sample.\n","            condition_column (string):\n","                Name of a discrete column.\n","            condition_value (string):\n","                Name of the category in the condition_column which we wish to increase the\n","                probability of happening.\n","\n","        Returns:\n","            numpy.ndarray or pandas.DataFrame\n","        \"\"\"\n","        if condition_column is not None and condition_value is not None:\n","            condition_info = self._transformer.convert_column_name_value_to_id(\n","                condition_column, condition_value)\n","            global_condition_vec = self._data_sampler.generate_cond_from_condition_column_info(\n","                condition_info, self._batch_size)\n","        else:\n","            global_condition_vec = None\n","\n","        steps = n // self._batch_size + 1\n","        data = []\n","        for i in range(steps):\n","            mean = torch.zeros(self._batch_size, self._embedding_dim)\n","            std = mean + 1\n","            fakez = torch.normal(mean=mean, std=std).to(self._device)\n","            latent_code = torch.rand(self._batch_size, self._latent_dim, device=self._device) * 2 - 1  # uniform 분포에서 latent code 생성\n","\n","            if global_condition_vec is not None:\n","                condvec = global_condition_vec.copy()\n","            else:\n","                condvec = self._data_sampler.sample_original_condvec(self._batch_size)\n","\n","            if condvec is None:\n","                pass\n","            else:\n","                c1 = condvec\n","                c1 = torch.from_numpy(c1).to(self._device)\n","                fakez = torch.cat([fakez, c1], dim=1)\n","\n","            fake = self._generator(fakez, latent_code) # latent code 추가\n","            fakeact = self._apply_activate(fake)\n","            data.append(fakeact.detach().cpu().numpy())\n","\n","        data = np.concatenate(data, axis=0)\n","        data = data[:n]\n","\n","        return self._transformer.inverse_transform(data)\n","\n","    def set_device(self, device):\n","        \"\"\"Set the `device` to be used ('GPU' or 'CPU).\"\"\"\n","        self._device = device\n","        if self._generator is not None:\n","            self._generator.to(self._device)\n","        if self._discriminator is not None:\n","            self._discriminator.to(self._device)\n","        if self._q_network is not None:\n","            self._q_network.to(self._device)\n"],"metadata":{"id":"g9h0Ulb3Usc6","executionInfo":{"status":"ok","timestamp":1716703010870,"user_tz":-540,"elapsed":15,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import f1_score\n","\n","data=pd.read_csv(\"/content/adult.csv\")\n","# 종속변수는 그냥 0,1로 설정\n","data['income'] = data['income'].replace({' <=50K': 0, ' >50K': 1})"],"metadata":{"id":"JPo0E-NzUdyY","executionInfo":{"status":"ok","timestamp":1716703010870,"user_tz":-540,"elapsed":15,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# 데이터 프레임을 훈련용과 테스트용으로 분리\n","train_df, test_df = train_test_split(real_data, test_size=0.3, random_state=42)"],"metadata":{"id":"weFCiDGCUk35","executionInfo":{"status":"ok","timestamp":1716703010871,"user_tz":-540,"elapsed":15,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# 훈련용 데이터로 샘플 생성\n","# Names of the columns that are discrete\n","discrete_columns = [\n","    'workclass',\n","    'education',\n","    'marital-status',\n","    'occupation',\n","    'relationship',\n","    'race',\n","    'sex',\n","    'native-country',\n","    'income'\n","]\n","\n","# CTGAN 모델 초기화\n","embedding_dim = 128\n","generator_dim = (256, 256)\n","discriminator_dim = (256, 256)\n","latent_dim = 1  # latent code의 차원\n","batch_size = 500\n","epochs = 100\n","cuda = True\n","pac = 10  # pac 값을 batch_size의 약수로 설정\n","\n","ctgan = CTGAN(\n","    embedding_dim=embedding_dim,\n","    generator_dim=generator_dim,\n","    discriminator_dim=discriminator_dim,\n","    latent_dim=latent_dim,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    pac=pac,  # pac 값을 설정\n","    cuda=cuda\n",")"],"metadata":{"id":"tdBuH7WvGhkz","executionInfo":{"status":"ok","timestamp":1716704356468,"user_tz":-540,"elapsed":2,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["ctgan.fit(train_df, discrete_columns)\n","# Create synthetic data\n","synthetic_data = ctgan.sample(10000)"],"metadata":{"id":"CvlvDlMfU7OY","executionInfo":{"status":"ok","timestamp":1716704531808,"user_tz":-540,"elapsed":174829,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["# 범주형 컬럼 get dummies 사용\n","synthetic_data_get_dummies=pd.get_dummies(synthetic_data)\n","test_df_get_dummies=pd.get_dummies(test_df)\n","\n","X_train=synthetic_data_get_dummies.drop('income',axis=1)\n","y_train=synthetic_data_get_dummies['income']\n","\n","X_test=test_df_get_dummies.drop('income',axis=1)\n","y_test=test_df_get_dummies['income']\n","\n","\n","# 범주형 컬럼에서 빈도가 적은 카테고리가 테스트에 포함되지 않을수도 있으니 그 카테고리 False로 차원 맞춰주기\n","if X_train.shape[1]>X_test.shape[1]:\n","  synthetic_data_get_dummies, test_df_get_dummies = synthetic_data_get_dummies.align(test_df_get_dummies, join='outer', axis=1, fill_value=False)\n","elif X_train.shape[1]<X_test.shape[1]:\n","  test_df_get_dummies, synthetic_data_get_dummies = test_df_get_dummies.align(synthetic_data_get_dummies, join='outer', axis=1, fill_value=False)\n","else:\n","  synthetic_data_get_dummies=pd.get_dummies(synthetic_data)\n","  test_df_get_dummies=pd.get_dummies(test_df)\n","\n","\n","X_train=synthetic_data_get_dummies.drop('income',axis=1)\n","y_train=synthetic_data_get_dummies['income']\n","\n","X_test=test_df_get_dummies.drop('income',axis=1)\n","y_test=test_df_get_dummies['income']"],"metadata":{"id":"hAdlE8BSVQyb","executionInfo":{"status":"ok","timestamp":1716704531809,"user_tz":-540,"elapsed":21,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rk88Qkf5VS06","executionInfo":{"status":"ok","timestamp":1716704531809,"user_tz":-540,"elapsed":9,"user":{"displayName":"허주혁","userId":"07084138917445903099"}},"outputId":"7593ecc1-5263-4238-9e93-edfb8bbe1775"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 108)\n","(10000,)\n","(9769, 108)\n","(9769,)\n"]}]},{"cell_type":"code","source":["# 의사결정 나무 모델 초기화 및 학습\n","clf = DecisionTreeClassifier(random_state=42)\n","clf.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"BKwMfgQ3VUJ8","executionInfo":{"status":"ok","timestamp":1716704531810,"user_tz":-540,"elapsed":9,"user":{"displayName":"허주혁","userId":"07084138917445903099"}},"outputId":"5364ee27-a39c-426c-c3d8-c09797b1554c"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DecisionTreeClassifier(random_state=42)"],"text/html":["<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["y_pred = clf.predict(X_test)"],"metadata":{"id":"NcjdtvYRVUaa","executionInfo":{"status":"ok","timestamp":1716704531810,"user_tz":-540,"elapsed":8,"user":{"displayName":"허주혁","userId":"07084138917445903099"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# latent_dim : 1, epochs : 100\n","# F1 스코어 계산\n","f1 = f1_score(y_test, y_pred)\n","print(\"F1 스코어:\", f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5q26Me7jZLa","executionInfo":{"status":"ok","timestamp":1716704531810,"user_tz":-540,"elapsed":8,"user":{"displayName":"허주혁","userId":"07084138917445903099"}},"outputId":"47a10cee-085a-4700-9bf3-347ae6b14482"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 스코어: 0.5445866782382296\n"]}]},{"cell_type":"code","source":["# latent_dim : 1, epochs : 100\n","# F1 스코어 계산\n","f1 = f1_score(y_test, y_pred)\n","print(\"F1 스코어:\", f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDyKZywIijbM","executionInfo":{"status":"ok","timestamp":1716704316318,"user_tz":-540,"elapsed":10,"user":{"displayName":"허주혁","userId":"07084138917445903099"}},"outputId":"b6d1a601-29f9-4004-8543-829aa265d9c9"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 스코어: 0.5606884057971016\n"]}]},{"cell_type":"code","source":["# # SVM\n","# from sklearn.svm import SVC\n","# # Linear SVM 모델 초기화 및 학습\n","# clf = SVC(kernel='linear', random_state=42)\n","# clf.fit(X_train, y_train)\n","\n","# # 테스트 데이터에 대한 예측 수행\n","# y_pred = clf.predict(X_test)\n","\n","# # F1 스코어 계산\n","# f1 = f1_score(y_test, y_pred)\n","\n","# print(\"F1 스코어:\", f1)"],"metadata":{"id":"bd6ffRtCVUj8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.neural_network import MLPClassifier\n","\n","# MLP 모델 초기화 및 학습\n","mlp = MLPClassifier(random_state=42, max_iter=500)\n","mlp.fit(X_train, y_train)\n","\n","# 테스트 데이터에 대한 예측 수행\n","y_pred = mlp.predict(X_test)\n","\n","# F1 스코어 계산\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"F1 스코어:\", f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gvc1wjaocNxT","executionInfo":{"status":"ok","timestamp":1716703677910,"user_tz":-540,"elapsed":1185,"user":{"displayName":"허주혁","userId":"07084138917445903099"}},"outputId":"786af43d-3676-486c-b0df-387f0672a934"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 스코어: 0.24621072088724585\n"]}]}]}