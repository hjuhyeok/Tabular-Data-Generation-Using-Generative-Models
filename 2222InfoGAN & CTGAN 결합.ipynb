{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciMSkNvvzIle"
   },
   "source": [
    "# CTGAN과 같은 모델에서 기본적으로 사용되는 기능들을 제공하는 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1759,
     "status": "ok",
     "timestamp": 1716703000529,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "wh2sLImPzHsx"
   },
   "outputs": [],
   "source": [
    "\"\"\"BaseSynthesizer module.\"\"\"\n",
    "\n",
    "import contextlib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def set_random_states(random_state, set_model_random_state):\n",
    "    \"\"\"랜덤 상태를 관리하기 위한 컨텍스트 매니저.\n",
    "    \n",
    "    인자:\n",
    "        random_state (int 또는 tuple):\n",
    "            랜덤 시드 혹은 (numpy.random.RandomState, torch.Generator) 튜플.\n",
    "        set_model_random_state (function):\n",
    "            모델의 랜덤 상태를 설정하는 함수.\n",
    "    \"\"\"\n",
    "    # 기존 numpy와 torch의 랜덤 상태를 저장\n",
    "    original_np_state = np.random.get_state()\n",
    "    original_torch_state = torch.get_rng_state()\n",
    "\n",
    "    # 전달받은 튜플에서 numpy와 torch의 랜덤 상태 객체를 분리\n",
    "    random_np_state, random_torch_state = random_state\n",
    "\n",
    "    # 전달받은 랜덤 상태를 적용 (각 객체의 내부 상태를 가져와서 설정)\n",
    "    np.random.set_state(random_np_state.get_state())\n",
    "    torch.set_rng_state(random_torch_state.get_state())\n",
    "\n",
    "    try:\n",
    "        yield  # 컨텍스트 블록 실행\n",
    "    finally:\n",
    "        # 컨텍스트 종료 시점에 현재의 numpy와 torch의 랜덤 상태를 새로 저장\n",
    "        current_np_state = np.random.RandomState()\n",
    "        current_np_state.set_state(np.random.get_state())\n",
    "        current_torch_state = torch.Generator()\n",
    "        current_torch_state.set_state(torch.get_rng_state())\n",
    "        # 모델의 랜덤 상태를 업데이트: 현재의 상태를 모델에 반영\n",
    "        set_model_random_state((current_np_state, current_torch_state))\n",
    "\n",
    "        # 원래 저장해두었던 랜덤 상태로 복구\n",
    "        np.random.set_state(original_np_state)\n",
    "        torch.set_rng_state(original_torch_state)\n",
    "\n",
    "def random_state(function):\n",
    "    \"\"\"함수를 호출하기 전에 랜덤 상태를 설정하는 데코레이터.\n",
    "    \n",
    "    인자:\n",
    "        function (Callable):\n",
    "            데코레이트할 함수.\n",
    "    \"\"\"\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        # 모델에 랜덤 상태가 설정되어 있지 않으면 그냥 함수 실행\n",
    "        if self.random_states is None:\n",
    "            return function(self, *args, **kwargs)\n",
    "        else:\n",
    "            # 모델에 설정된 랜덤 상태를 적용한 상태로 함수 실행\n",
    "            with set_random_states(self.random_states, self.set_random_state):\n",
    "                return function(self, *args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "class BaseSynthesizer:\n",
    "    \"\"\"CTGAN의 기본 합성기(Base Synthesizer) 클래스.\n",
    "    \n",
    "    CTGAN의 모든 기본 합성기(데이터 생성기)의 공통 기능을 제공.\n",
    "    \"\"\"\n",
    "    # 클래스 변수: 모델의 랜덤 상태를 저장 (없으면 None)\n",
    "    random_states = None\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\"객체를 피클링(pickle)할 때의 상태를 개선합니다.\n",
    "        \n",
    "        피클링 전에 CPU 디바이스로 변환하여 저장하고,\n",
    "        random_states가 설정된 경우엔 그 상태를 generator가 아닌 dict 형식으로 저장합니다.\n",
    "        \n",
    "        반환:\n",
    "            dict: 객체 상태를 나타내는 파이썬 딕셔너리.\n",
    "        \"\"\"\n",
    "        # 현재 사용 중인 디바이스를 백업\n",
    "        device_backup = self._device\n",
    "        # 피클링 전에 CPU 디바이스로 변경\n",
    "        self.set_device(torch.device('cpu'))\n",
    "        # 객체의 상태를 딕셔너리로 복사\n",
    "        state = self.__dict__.copy()\n",
    "        # 원래 디바이스로 복구\n",
    "        self.set_device(device_backup)\n",
    "        # 만약 random_states가 (numpy.random.RandomState, torch.Generator) 튜플이면,\n",
    "        # 각각의 상태를 딕셔너리로 저장하고 기존의 random_states 키는 제거\n",
    "        if (\n",
    "            isinstance(self.random_states, tuple) and\n",
    "            isinstance(self.random_states[0], np.random.RandomState) and\n",
    "            isinstance(self.random_states[1], torch.Generator)\n",
    "        ):\n",
    "            state['_numpy_random_state'] = self.random_states[0].get_state()\n",
    "            state['_torch_random_state'] = self.random_states[1].get_state()\n",
    "            state.pop('random_states')\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        \"\"\"피클링된 객체의 상태를 복원합니다.\n",
    "        \n",
    "        상태 딕셔너리에 저장된 random_states가 있으면 복원하고,\n",
    "        현재 하드웨어에 맞게 디바이스를 설정합니다.\n",
    "        \"\"\"\n",
    "        # 만약 피클된 상태에 랜덤 상태가 저장되어 있으면 복원\n",
    "        if '_numpy_random_state' in state and '_torch_random_state' in state:\n",
    "            np_state = state.pop('_numpy_random_state')\n",
    "            torch_state = state.pop('_torch_random_state')\n",
    "            \n",
    "            # 새 torch.Generator를 생성하고, 저장된 상태를 설정\n",
    "            current_torch_state = torch.Generator()\n",
    "            current_torch_state.set_state(torch_state)\n",
    "            \n",
    "            # 새 numpy.random.RandomState를 생성하고, 저장된 상태를 설정\n",
    "            current_numpy_state = np.random.RandomState()\n",
    "            current_numpy_state.set_state(np_state)\n",
    "            state['random_states'] = (current_numpy_state, current_torch_state)\n",
    "        \n",
    "        # 복원된 상태를 객체에 할당\n",
    "        self.__dict__ = state\n",
    "        # 사용 가능한 디바이스(GPU/CPU)에 따라 디바이스 설정\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.set_device(device)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"모델을 주어진 경로에 저장합니다.\n",
    "        \n",
    "        저장 전에 CPU 디바이스로 전환하여, 다양한 환경에서 로드할 수 있도록 합니다.\n",
    "        \n",
    "        인자:\n",
    "            path (str): 저장할 파일 경로.\n",
    "        \"\"\"\n",
    "        device_backup = self._device  # 현재 디바이스 백업\n",
    "        self.set_device(torch.device('cpu'))  # CPU 모드로 전환\n",
    "        torch.save(self, path)  # 모델 저장\n",
    "        self.set_device(device_backup)  # 원래 디바이스로 복구\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"주어진 경로에서 저장된 모델을 불러옵니다.\n",
    "        \n",
    "        인자:\n",
    "            path (str): 저장된 모델 파일 경로.\n",
    "        \n",
    "        반환:\n",
    "            모델 객체: 불러온 모델.\n",
    "        \"\"\"\n",
    "        # 사용 가능한 디바이스에 따라 설정\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model = torch.load(path)  # 모델 불러오기\n",
    "        model.set_device(device)  # 디바이스 재설정\n",
    "        return model\n",
    "\n",
    "    def set_random_state(self, random_state):\n",
    "        \"\"\"랜덤 상태를 설정합니다.\n",
    "        \n",
    "        인자:\n",
    "            random_state (int, tuple, 또는 None):\n",
    "                - int: numpy와 torch에 동일한 시드를 부여하여 생성.\n",
    "                - tuple: (numpy.random.RandomState, torch.Generator) 형태.\n",
    "                - None: 랜덤 상태를 설정하지 않음.\n",
    "        \"\"\"\n",
    "        if random_state is None:\n",
    "            self.random_states = random_state\n",
    "        elif isinstance(random_state, int):\n",
    "            # 정수 시드를 사용하여 numpy와 torch의 난수 생성기를 초기화\n",
    "            self.random_states = (\n",
    "                np.random.RandomState(seed=random_state),\n",
    "                torch.Generator().manual_seed(random_state),\n",
    "            )\n",
    "        elif (\n",
    "            isinstance(random_state, tuple) and\n",
    "            isinstance(random_state[0], np.random.RandomState) and\n",
    "            isinstance(random_state[1], torch.Generator)\n",
    "        ):\n",
    "            self.random_states = random_state  # 튜플 그대로 설정\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f'`random_state` {random_state} expected to be an int or a tuple of '\n",
    "                '(`np.random.RandomState`, `torch.Generator`)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-UGf5A1rZfQ"
   },
   "source": [
    "# DataSampler 함수 (이산형 변수 랜덤하게 선택한 후 로그확률로 카테고리 선택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1716703000530,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "9nR_-H5-rTwb"
   },
   "outputs": [],
   "source": [
    "\"\"\"DataSampler module.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DataSampler(object):\n",
    "    \"\"\"\n",
    "    DataSampler는 CTGAN에서 조건부 벡터(conditional vector)와 이에 대응하는 데이터를 샘플링하는 역할을 합니다.\n",
    "    이 모듈은 주어진 학습 데이터와 출력 정보(output_info)를 바탕으로,\n",
    "    각 이산형(범주형) 컬럼별로 데이터 행 인덱스와 카테고리 확률을 미리 계산하여 저장하고,\n",
    "    학습 또는 샘플링 시 조건부 벡터를 생성할 수 있도록 돕습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, output_info, log_frequency):\n",
    "        \"\"\"\n",
    "        DataSampler 초기화.\n",
    "        \n",
    "        인자:\n",
    "            data (np.array): 학습 데이터 (2차원 넘파이 배열).\n",
    "            output_info (list): 각 컬럼(열)의 변환 정보를 담은 리스트.\n",
    "            log_frequency (bool): 이산형 변수 샘플링 시 로그 확률을 사용할지 여부.\n",
    "        \"\"\"\n",
    "        self._data_length = len(data)  # 학습 데이터의 전체 행 개수 저장\n",
    "\n",
    "        # 주어진 컬럼 정보가 이산형(범주형)인지 판별하는 함수\n",
    "        def is_discrete_column(column_info):\n",
    "            # 이산형 컬럼은 column_info가 하나의 원소를 가지며, 해당 원소의 활성화 함수가 'softmax'인 경우로 정의\n",
    "            return (len(column_info) == 1 and column_info[0].activation_fn == 'softmax')\n",
    "\n",
    "        # output_info에서 이산형 컬럼의 개수를 계산\n",
    "        n_discrete_columns = sum(\n",
    "            [1 for column_info in output_info if is_discrete_column(column_info)]\n",
    "        )\n",
    "\n",
    "        # 각 이산형 컬럼에 대한 조건 벡터의 시작 인덱스를 저장할 배열 초기화\n",
    "        self._discrete_column_matrix_st = np.zeros(n_discrete_columns, dtype='int32')\n",
    "\n",
    "        # _rid_by_cat_cols: 각 이산형 컬럼의 각 카테고리에 해당하는 행(row) 인덱스를 저장하는 리스트\n",
    "        # 예: _rid_by_cat_cols[a][b]는 a번째 이산형 컬럼에서 값이 b인 모든 행의 인덱스 리스트\n",
    "        self._rid_by_cat_cols = []\n",
    "\n",
    "        # output_info를 기반으로 각 컬럼별로 행 인덱스를 저장\n",
    "        st = 0  # 시작 인덱스\n",
    "        for column_info in output_info:\n",
    "            if is_discrete_column(column_info):\n",
    "                span_info = column_info[0]\n",
    "                ed = st + span_info.dim  # 이 컬럼의 끝 인덱스\n",
    "\n",
    "                rid_by_cat = []\n",
    "                # 각 카테고리에 대해 해당 열에서 값이 1인 행들의 인덱스 찾기\n",
    "                for j in range(span_info.dim):\n",
    "                    rid_by_cat.append(np.nonzero(data[:, st + j])[0])\n",
    "                self._rid_by_cat_cols.append(rid_by_cat)\n",
    "                st = ed  # 다음 컬럼을 위해 인덱스 업데이트\n",
    "            else:\n",
    "                # 연속형 변수인 경우, 해당 컬럼의 전체 차원만큼 인덱스 증가\n",
    "                st += sum([span_info.dim for span_info in column_info])\n",
    "        # 최종적으로 계산된 인덱스가 데이터의 열 수와 일치해야 함을 확인\n",
    "        assert st == data.shape[1]\n",
    "\n",
    "        # 조건 벡터를 샘플링할 때 사용할 카테고리 확률 정보를 저장할 배열들을 초기화\n",
    "        # 최대 카테고리 수를 구함 (여러 이산형 컬럼 중 가장 큰 카테고리 수)\n",
    "        max_category = max([\n",
    "            column_info[0].dim\n",
    "            for column_info in output_info\n",
    "            if is_discrete_column(column_info)\n",
    "        ], default=0)\n",
    "\n",
    "        # 각 이산형 컬럼의 조건 벡터 시작 인덱스를 저장\n",
    "        self._discrete_column_cond_st = np.zeros(n_discrete_columns, dtype='int32')\n",
    "        # 각 이산형 컬럼의 카테고리 수를 저장\n",
    "        self._discrete_column_n_category = np.zeros(n_discrete_columns, dtype='int32')\n",
    "        # 각 이산형 컬럼의 카테고리별 확률을 저장하는 배열 (n_discrete_columns x max_category)\n",
    "        self._discrete_column_category_prob = np.zeros((n_discrete_columns, max_category))\n",
    "        self._n_discrete_columns = n_discrete_columns  # 전체 이산형 컬럼 수\n",
    "        # 전체 카테고리 수 (모든 이산형 컬럼의 카테고리 수의 합)\n",
    "        self._n_categories = sum([\n",
    "            column_info[0].dim\n",
    "            for column_info in output_info\n",
    "            if is_discrete_column(column_info)\n",
    "        ])\n",
    "\n",
    "        # 각 이산형 컬럼에 대해 카테고리 발생 빈도 및 확률 계산\n",
    "        st = 0\n",
    "        current_id = 0  # 이산형 컬럼 인덱스 카운터\n",
    "        current_cond_st = 0  # 조건 벡터 상에서의 시작 인덱스\n",
    "        for column_info in output_info:\n",
    "            if is_discrete_column(column_info):\n",
    "                span_info = column_info[0]\n",
    "                ed = st + span_info.dim\n",
    "                # 해당 컬럼의 각 카테고리 발생 빈도 계산 (열 단위 합산)\n",
    "                category_freq = np.sum(data[:, st:ed], axis=0)\n",
    "                if log_frequency:\n",
    "                    # log_frequency가 True이면, 로그 변환을 적용하여 확률 계산에 사용\n",
    "                    category_freq = np.log(category_freq + 1)\n",
    "                # 카테고리별 확률로 정규화\n",
    "                category_prob = category_freq / np.sum(category_freq)\n",
    "                # 계산된 확률을 저장 (해당 컬럼의 차원만큼)\n",
    "                self._discrete_column_category_prob[current_id, :span_info.dim] = category_prob\n",
    "                # 조건 벡터에서 해당 컬럼의 시작 인덱스 저장\n",
    "                self._discrete_column_cond_st[current_id] = current_cond_st\n",
    "                # 해당 컬럼의 카테고리 수 저장\n",
    "                self._discrete_column_n_category[current_id] = span_info.dim\n",
    "                # 다음 조건 벡터 시작 인덱스 업데이트\n",
    "                current_cond_st += span_info.dim\n",
    "                current_id += 1\n",
    "                st = ed\n",
    "            else:\n",
    "                st += sum([span_info.dim for span_info in column_info])\n",
    "\n",
    "    def _random_choice_prob_index(self, discrete_column_id):\n",
    "        \"\"\"\n",
    "        주어진 이산형 컬럼에 대해 미리 계산된 카테고리 확률에 따라 확률적 샘플링을 수행합니다.\n",
    "        \n",
    "        인자:\n",
    "            discrete_column_id (int): 샘플링할 이산형 컬럼의 인덱스.\n",
    "        \n",
    "        반환:\n",
    "            각 샘플에 대해 선택된 카테고리 인덱스 배열.\n",
    "        \"\"\"\n",
    "        probs = self._discrete_column_category_prob[discrete_column_id]\n",
    "        # 각 카테고리에 대해 누적 확률을 구하기 위한 난수 생성 (각 카테고리마다 난수 생성 후 차원 확장)\n",
    "        r = np.expand_dims(np.random.rand(probs.shape[0]), axis=1)\n",
    "        # 누적 확률이 난수보다 커지는 첫 번째 인덱스를 선택\n",
    "        return (probs.cumsum(axis=1) > r).argmax(axis=1)\n",
    "\n",
    "    def sample_condvec(self, batch):\n",
    "        \"\"\"\n",
    "        학습을 위한 조건 벡터(conditional vector)를 생성합니다.\n",
    "        \n",
    "        반환:\n",
    "            cond (배치 x 전체 카테고리 수): 조건 벡터. 각 샘플마다 한 카테고리에 해당하는 위치에 1이 있음.\n",
    "            mask (배치 x 이산형 컬럼 수): 각 샘플이 어떤 이산형 컬럼을 선택했는지를 나타내는 one-hot 벡터.\n",
    "            discrete_column_id (배치): 각 샘플에 대해 선택된 이산형 컬럼의 ID.\n",
    "            category_id_in_col (배치): 선택된 이산형 컬럼 내에서의 카테고리 인덱스.\n",
    "        \"\"\"\n",
    "        if self._n_discrete_columns == 0:\n",
    "            return None\n",
    "\n",
    "        # 배치 내 각 샘플마다 무작위로 하나의 이산형 컬럼 선택\n",
    "        discrete_column_id = np.random.choice(\n",
    "            np.arange(self._n_discrete_columns), batch\n",
    "        )\n",
    "\n",
    "        # 조건 벡터와 마스크 초기화\n",
    "        cond = np.zeros((batch, self._n_categories), dtype='float32')\n",
    "        mask = np.zeros((batch, self._n_discrete_columns), dtype='float32')\n",
    "        # 선택된 이산형 컬럼을 one-hot 방식으로 표시\n",
    "        mask[np.arange(batch), discrete_column_id] = 1\n",
    "        # 각 선택된 이산형 컬럼에 대해 카테고리 인덱스를 확률적으로 샘플링\n",
    "        category_id_in_col = self._random_choice_prob_index(discrete_column_id)\n",
    "        # 실제 전체 조건 벡터 상의 인덱스 계산:\n",
    "        # 각 이산형 컬럼의 조건 벡터 시작 인덱스에 선택된 카테고리 인덱스를 더함\n",
    "        category_id = (self._discrete_column_cond_st[discrete_column_id] + category_id_in_col)\n",
    "        # 조건 벡터에서 해당 인덱스 위치에 1 할당\n",
    "        cond[np.arange(batch), category_id] = 1\n",
    "\n",
    "        return cond, mask, discrete_column_id, category_id_in_col\n",
    "\n",
    "    def sample_original_condvec(self, batch):\n",
    "        \"\"\"\n",
    "        원본 데이터 분포를 기반으로 조건 벡터를 샘플링합니다.\n",
    "        이 방식은 전체 데이터의 카테고리 분포를 반영합니다.\n",
    "        \n",
    "        인자:\n",
    "            batch (int): 배치 당 샘플 수.\n",
    "        \n",
    "        반환:\n",
    "            cond (배치 x 전체 카테고리 수): 샘플링된 조건 벡터.\n",
    "        \"\"\"\n",
    "        if self._n_discrete_columns == 0:\n",
    "            return None\n",
    "\n",
    "        # 전체 이산형 컬럼의 카테고리 확률을 1차원 배열로 변환 후, 0이 아닌 값만 선택\n",
    "        category_freq = self._discrete_column_category_prob.flatten()\n",
    "        category_freq = category_freq[category_freq != 0]\n",
    "        # 정규화: 확률의 합이 1이 되도록 조정\n",
    "        category_freq = category_freq / np.sum(category_freq)\n",
    "        # 확률 분포를 기반으로 배치마다 카테고리 인덱스 샘플링\n",
    "        col_idxs = np.random.choice(np.arange(len(category_freq)), batch, p=category_freq)\n",
    "        # 조건 벡터 초기화\n",
    "        cond = np.zeros((batch, self._n_categories), dtype='float32')\n",
    "        # 각 샘플에 대해 선택된 인덱스 위치에 1 할당\n",
    "        cond[np.arange(batch), col_idxs] = 1\n",
    "\n",
    "        return cond\n",
    "\n",
    "    def sample_data(self, data, n, col, opt):\n",
    "        \"\"\"\n",
    "        주어진 조건 벡터를 만족하는 원본 데이터를 샘플링합니다.\n",
    "        \n",
    "        인자:\n",
    "            data (np.array): 원본 학습 데이터.\n",
    "            n (int): 샘플링할 행의 개수.\n",
    "            col (list): 선택된 이산형 컬럼 정보 (컬럼 인덱스 리스트).\n",
    "            opt (list): 각 컬럼에서 선택된 카테고리 인덱스 리스트.\n",
    "        \n",
    "        반환:\n",
    "            np.array: 조건을 만족하는 n개의 데이터 행.\n",
    "        \"\"\"\n",
    "        # 만약 조건(col)이 없다면, 무작위 인덱스로 데이터 샘플링\n",
    "        if col is None:\n",
    "            idx = np.random.randint(len(data), size=n)\n",
    "            return data[idx]\n",
    "\n",
    "        idx = []\n",
    "        # 각 이산형 컬럼과 선택된 카테고리에 대해, 해당하는 행 인덱스 중 무작위 선택\n",
    "        for c, o in zip(col, opt):\n",
    "            idx.append(np.random.choice(self._rid_by_cat_cols[c][o]))\n",
    "        return data[idx]\n",
    "\n",
    "    def dim_cond_vec(self):\n",
    "        \"\"\"\n",
    "        조건 벡터의 총 차원(전체 카테고리 수)을 반환합니다.\n",
    "        이 값은 Generator에 조건 벡터 크기를 맞추기 위해 사용됩니다.\n",
    "        \"\"\"\n",
    "        return self._n_categories\n",
    "\n",
    "    def generate_cond_from_condition_column_info(self, condition_info, batch):\n",
    "        \"\"\"\n",
    "        주어진 조건 정보를 기반으로 조건 벡터를 생성합니다.\n",
    "        \n",
    "        인자:\n",
    "            condition_info (dict): 특정 컬럼과 값에 대한 정보.\n",
    "                예: {'discrete_column_id': x, 'value_id': y}\n",
    "            batch (int): 생성할 조건 벡터의 배치 크기.\n",
    "        \n",
    "        반환:\n",
    "            vec (np.array): 조건 벡터 (배치 x 전체 카테고리 수).\n",
    "        \"\"\"\n",
    "        vec = np.zeros((batch, self._n_categories), dtype='float32')\n",
    "        # condition_info를 기반으로 이산형 컬럼의 조건 벡터 시작 인덱스에서 value_id를 더해\n",
    "        # 조건 벡터 내에서 해당 위치를 찾음\n",
    "        id_ = self._discrete_column_matrix_st[condition_info['discrete_column_id']]\n",
    "        id_ += condition_info['value_id']\n",
    "        vec[:, id_] = 1\n",
    "        return vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZnhCns5vK6s"
   },
   "source": [
    "# 연속형 변수에 대해서 GMM으로 [-1.1]로 정규화 한 후 Tanh로 변환\n",
    "# 이산형 변수에 대해서 Onehotencoder사용해 원-핫 벡터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8688,
     "status": "ok",
     "timestamp": 1716703009209,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "9O_EBEUp71gf",
    "outputId": "e24be962-a2e5-4704-cbcf-6c0e18b175e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdt in /usr/local/lib/python3.10/dist-packages (1.12.1)\n",
      "Requirement already satisfied: Faker>=17 in /usr/local/lib/python3.10/dist-packages (from rdt) (25.2.0)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from rdt) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from rdt) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.10/dist-packages (from rdt) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from rdt) (1.11.4)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from Faker>=17->rdt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->rdt) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->rdt) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->Faker>=17->rdt) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1668,
     "status": "ok",
     "timestamp": 1716703010868,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "SxX0MU6gvJht"
   },
   "outputs": [],
   "source": [
    "\"\"\"DataTransformer module.\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from rdt.transformers import ClusterBasedNormalizer, OneHotEncoder\n",
    "\n",
    "# 각 컬럼의 변환 결과에서 출력 차원(dim)과 활성화 함수 정보를 저장하는 namedtuple\n",
    "SpanInfo = namedtuple('SpanInfo', ['dim', 'activation_fn'])\n",
    "# 각 컬럼에 대한 변환 정보 (컬럼 이름, 컬럼 타입, 변환기 객체, 출력 정보, 최종 출력 차원)를 저장하는 namedtuple\n",
    "ColumnTransformInfo = namedtuple(\n",
    "    'ColumnTransformInfo', [\n",
    "        'column_name', 'column_type', 'transform', 'output_info', 'output_dimensions'\n",
    "    ]\n",
    ")\n",
    "\n",
    "class DataTransformer(object):\n",
    "    \"\"\"\n",
    "    DataTransformer 클래스는 CTGAN에서 사용할 데이터를 전처리하기 위한 모듈입니다.\n",
    "    \n",
    "    연속형(continuous) 컬럼은 Bayesian GMM(ClusterBasedNormalizer)을 사용하여 정규화하고,\n",
    "    [-1, 1] 범위의 스칼라 값과 one-hot 벡터를 생성합니다.\n",
    "    이산형(discrete) 컬럼은 OneHotEncoder를 사용하여 원-핫 인코딩 방식으로 변환합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_clusters=10, weight_threshold=0.005):\n",
    "        \"\"\"\n",
    "        DataTransformer 객체를 초기화합니다.\n",
    "        \n",
    "        Args:\n",
    "            max_clusters (int):\n",
    "                Bayesian GMM에서 사용할 최대 Gaussian 분포 개수.\n",
    "            weight_threshold (float):\n",
    "                Gaussian 분포를 유지하기 위한 최소 가중치 임계값.\n",
    "        \"\"\"\n",
    "        self._max_clusters = max_clusters\n",
    "        self._weight_threshold = weight_threshold\n",
    "\n",
    "    def _fit_continuous(self, data):\n",
    "        \"\"\"\n",
    "        연속형 컬럼에 대해 Bayesian GMM 기반 정규화 모델을 학습합니다.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame):\n",
    "                변환할 하나의 연속형 컬럼을 포함한 DataFrame.\n",
    "                \n",
    "        Returns:\n",
    "            ColumnTransformInfo:\n",
    "                학습된 변환 정보를 담은 namedtuple.\n",
    "                출력 정보는 첫 번째 값은 정규화된 스칼라(tanh 활성화)이고,\n",
    "                두 번째 값은 해당 컬럼이 어느 Gaussian 컴포넌트에 속하는지를 나타내는 one-hot 벡터(softmax 활성화)입니다.\n",
    "        \"\"\"\n",
    "        column_name = data.columns[0]\n",
    "        gm = ClusterBasedNormalizer(\n",
    "            missing_value_generation='from_column',\n",
    "            max_clusters=min(len(data), self._max_clusters),\n",
    "            weight_threshold=self._weight_threshold\n",
    "        )\n",
    "        gm.fit(data, column_name)\n",
    "        # 유효한 Gaussian 컴포넌트의 수 계산\n",
    "        num_components = sum(gm.valid_component_indicator)\n",
    "\n",
    "        return ColumnTransformInfo(\n",
    "            column_name=column_name,\n",
    "            column_type='continuous',\n",
    "            transform=gm,\n",
    "            output_info=[SpanInfo(1, 'tanh'), SpanInfo(num_components, 'softmax')],\n",
    "            output_dimensions=1 + num_components\n",
    "        )\n",
    "\n",
    "    def _fit_discrete(self, data):\n",
    "        \"\"\"\n",
    "        이산형 컬럼에 대해 OneHotEncoder를 학습시킵니다.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame):\n",
    "                변환할 하나의 이산형 컬럼을 포함한 DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            ColumnTransformInfo:\n",
    "                학습된 OneHotEncoder와 변환 정보를 담은 namedtuple.\n",
    "                출력 정보는 해당 컬럼의 카테고리 수와 softmax 활성화 함수를 사용합니다.\n",
    "        \"\"\"\n",
    "        column_name = data.columns[0]\n",
    "        ohe = OneHotEncoder()\n",
    "        ohe.fit(data, column_name)\n",
    "        num_categories = len(ohe.dummies)\n",
    "\n",
    "        return ColumnTransformInfo(\n",
    "            column_name=column_name,\n",
    "            column_type='discrete',\n",
    "            transform=ohe,\n",
    "            output_info=[SpanInfo(num_categories, 'softmax')],\n",
    "            output_dimensions=num_categories\n",
    "        )\n",
    "\n",
    "    def fit(self, raw_data, discrete_columns=()):\n",
    "        \"\"\"\n",
    "        DataTransformer를 학습시켜, 연속형 컬럼과 이산형 컬럼의 변환기를 구성합니다.\n",
    "        \n",
    "        연속형 컬럼은 Bayesian GMM을, 이산형 컬럼은 OneHotEncoder를 사용합니다.\n",
    "        이 과정에서 전체 출력 차원과 각 컬럼의 span 정보를 계산합니다.\n",
    "        \n",
    "        Args:\n",
    "            raw_data (pd.DataFrame 또는 np.array):\n",
    "                변환할 원본 데이터.\n",
    "            discrete_columns (list):\n",
    "                이산형 변수로 처리할 컬럼 목록 (DataFrame인 경우 컬럼 이름, numpy array인 경우 인덱스).\n",
    "        \"\"\"\n",
    "        self.output_info_list = []\n",
    "        self.output_dimensions = 0\n",
    "        self.dataframe = True\n",
    "\n",
    "        # raw_data가 DataFrame이 아닌 경우 DataFrame으로 변환\n",
    "        if not isinstance(raw_data, pd.DataFrame):\n",
    "            self.dataframe = False\n",
    "            # 숫자형 컬럼 이름 문제 해결을 위해 모든 컬럼 이름을 문자열로 변환\n",
    "            discrete_columns = [str(column) for column in discrete_columns]\n",
    "            column_names = [str(num) for num in range(raw_data.shape[1])]\n",
    "            raw_data = pd.DataFrame(raw_data, columns=column_names)\n",
    "\n",
    "        # 각 컬럼의 원래 데이터 타입 저장\n",
    "        self._column_raw_dtypes = raw_data.infer_objects().dtypes\n",
    "        self._column_transform_info_list = []\n",
    "        # 각 컬럼별로 변환기를 학습 (이산형 컬럼이면 _fit_discrete, 연속형이면 _fit_continuous)\n",
    "        for column_name in raw_data.columns:\n",
    "            if column_name in discrete_columns:\n",
    "                column_transform_info = self._fit_discrete(raw_data[[column_name]])\n",
    "            else:\n",
    "                column_transform_info = self._fit_continuous(raw_data[[column_name]])\n",
    "\n",
    "            self.output_info_list.append(column_transform_info.output_info)\n",
    "            self.output_dimensions += column_transform_info.output_dimensions\n",
    "            self._column_transform_info_list.append(column_transform_info)\n",
    "\n",
    "    def _transform_continuous(self, column_transform_info, data):\n",
    "        \"\"\"\n",
    "        연속형 컬럼 데이터를 변환하여 CTGAN 학습에 적합한 형식으로 만듭니다.\n",
    "        \n",
    "        Args:\n",
    "            column_transform_info (ColumnTransformInfo):\n",
    "                해당 컬럼의 변환 정보.\n",
    "            data (pd.DataFrame):\n",
    "                변환할 데이터를 포함한 DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: 변환된 데이터를 담은 넘파이 배열.\n",
    "        \"\"\"\n",
    "        column_name = data.columns[0]\n",
    "        # 컬럼을 1차원 배열로 평탄화\n",
    "        flattened_column = data[column_name].to_numpy().flatten()\n",
    "        data = data.assign(**{column_name: flattened_column})\n",
    "        gm = column_transform_info.transform\n",
    "        transformed = gm.transform(data)\n",
    "\n",
    "        # 출력 배열 초기화: 첫 번째 열은 정규화된 값, 나머지는 Gaussian 컴포넌트 정보를 one-hot 인코딩한 값\n",
    "        output = np.zeros((len(transformed), column_transform_info.output_dimensions))\n",
    "        output[:, 0] = transformed[f'{column_name}.normalized'].to_numpy()\n",
    "        index = transformed[f'{column_name}.component'].to_numpy().astype(int)\n",
    "        output[np.arange(index.size), index + 1] = 1.0\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _transform_discrete(self, column_transform_info, data):\n",
    "        \"\"\"\n",
    "        이산형 데이터를 One-Hot Encoding 방식으로 변환합니다.\n",
    "        \n",
    "        Args:\n",
    "            column_transform_info (ColumnTransformInfo):\n",
    "                해당 컬럼의 변환 정보.\n",
    "            data (pd.DataFrame):\n",
    "                변환할 데이터를 포함한 DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: 원-핫 인코딩된 넘파이 배열.\n",
    "        \"\"\"\n",
    "        ohe = column_transform_info.transform\n",
    "        return ohe.transform(data).to_numpy()\n",
    "\n",
    "    def _synchronous_transform(self, raw_data, column_transform_info_list):\n",
    "        \"\"\"\n",
    "        Pandas DataFrame의 각 컬럼을 순차적으로 변환합니다.\n",
    "        \n",
    "        Args:\n",
    "            raw_data (pd.DataFrame): 변환할 데이터.\n",
    "            column_transform_info_list (list): 각 컬럼의 변환 정보 리스트.\n",
    "        \n",
    "        Returns:\n",
    "            list: 각 컬럼별 변환된 넘파이 배열의 리스트.\n",
    "        \"\"\"\n",
    "        column_data_list = []\n",
    "        for column_transform_info in column_transform_info_list:\n",
    "            column_name = column_transform_info.column_name\n",
    "            data = raw_data[[column_name]]\n",
    "            if column_transform_info.column_type == 'continuous':\n",
    "                column_data_list.append(self._transform_continuous(column_transform_info, data))\n",
    "            else:\n",
    "                column_data_list.append(self._transform_discrete(column_transform_info, data))\n",
    "        return column_data_list\n",
    "\n",
    "    def _parallel_transform(self, raw_data, column_transform_info_list):\n",
    "        \"\"\"\n",
    "        Pandas DataFrame의 각 컬럼을 병렬적으로 변환합니다.\n",
    "        \n",
    "        Args:\n",
    "            raw_data (pd.DataFrame): 변환할 데이터.\n",
    "            column_transform_info_list (list): 각 컬럼의 변환 정보 리스트.\n",
    "        \n",
    "        Returns:\n",
    "            list: 각 컬럼별 변환된 넘파이 배열의 리스트.\n",
    "        \"\"\"\n",
    "        processes = []\n",
    "        for column_transform_info in column_transform_info_list:\n",
    "            column_name = column_transform_info.column_name\n",
    "            data = raw_data[[column_name]]\n",
    "            process = None\n",
    "            if column_transform_info.column_type == 'continuous':\n",
    "                process = delayed(self._transform_continuous)(column_transform_info, data)\n",
    "            else:\n",
    "                process = delayed(self._transform_discrete)(column_transform_info, data)\n",
    "            processes.append(process)\n",
    "        return Parallel(n_jobs=-1)(processes)\n",
    "\n",
    "    def transform(self, raw_data):\n",
    "        \"\"\"\n",
    "        원본 데이터를 변환하여 CTGAN 학습에 사용할 행렬 형태의 데이터로 출력합니다.\n",
    "        \n",
    "        Args:\n",
    "            raw_data (pd.DataFrame 또는 np.array): 변환할 원본 데이터.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: 모든 컬럼을 변환한 후 연결한 행렬 데이터.\n",
    "        \"\"\"\n",
    "        if not isinstance(raw_data, pd.DataFrame):\n",
    "            column_names = [str(num) for num in range(raw_data.shape[1])]\n",
    "            raw_data = pd.DataFrame(raw_data, columns=column_names)\n",
    "\n",
    "        # 데이터 크기가 작으면 순차적 변환, 크면 병렬 변환을 사용\n",
    "        if raw_data.shape[0] < 500:\n",
    "            column_data_list = self._synchronous_transform(\n",
    "                raw_data, self._column_transform_info_list\n",
    "            )\n",
    "        else:\n",
    "            column_data_list = self._parallel_transform(\n",
    "                raw_data, self._column_transform_info_list\n",
    "            )\n",
    "\n",
    "        # 모든 컬럼의 변환 결과를 하나의 넘파이 배열로 연결\n",
    "        return np.concatenate(column_data_list, axis=1).astype(float)\n",
    "\n",
    "    def _inverse_transform_continuous(self, column_transform_info, column_data, sigmas, st):\n",
    "        \"\"\"\n",
    "        연속형 데이터를 원본 형식으로 복원합니다.\n",
    "        \n",
    "        Args:\n",
    "            column_transform_info (ColumnTransformInfo): 해당 컬럼의 변환 정보.\n",
    "            column_data (np.array): 변환된 데이터.\n",
    "            sigmas (list 또는 None): 복원 시 사용될 표준편차 값들.\n",
    "            st (int): 현재 컬럼에 해당하는 시점(인덱스) 정보.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: 복원된 데이터.\n",
    "        \"\"\"\n",
    "        gm = column_transform_info.transform\n",
    "        # 변환된 데이터를 DataFrame으로 변환 (정규화된 값과 컴포넌트 정보를 포함)\n",
    "        data = pd.DataFrame(\n",
    "            column_data[:, :2], columns=list(gm.get_output_sdtypes())\n",
    "        ).astype(float)\n",
    "        # 컴포넌트 정보는 원-핫 인코딩 형태이므로, argmax를 통해 선택된 컴포넌트 인덱스 추출\n",
    "        data[data.columns[1]] = np.argmax(column_data[:, 1:], axis=1)\n",
    "        if sigmas is not None:\n",
    "            # 표준편차를 사용해 선택된 정규화된 값에 노이즈 추가 (복원 과정에서 변동성 부여)\n",
    "            selected_normalized_value = np.random.normal(data.iloc[:, 0], sigmas[st])\n",
    "            data.iloc[:, 0] = selected_normalized_value\n",
    "\n",
    "        # 원래 데이터 형식으로 복원\n",
    "        return gm.reverse_transform(data)\n",
    "\n",
    "    def _inverse_transform_discrete(self, column_transform_info, column_data):\n",
    "        \"\"\"\n",
    "        이산형 데이터를 원래 범주형 값으로 복원합니다.\n",
    "        \n",
    "        Args:\n",
    "            column_transform_info (ColumnTransformInfo): 해당 컬럼의 변환 정보.\n",
    "            column_data (np.array): 변환된 데이터.\n",
    "        \n",
    "        Returns:\n",
    "            pd.Series: 복원된 범주형 데이터.\n",
    "        \"\"\"\n",
    "        ohe = column_transform_info.transform\n",
    "        data = pd.DataFrame(column_data, columns=list(ohe.get_output_sdtypes()))\n",
    "        return ohe.reverse_transform(data)[column_transform_info.column_name]\n",
    "\n",
    "    def inverse_transform(self, data, sigmas=None):\n",
    "        \"\"\"\n",
    "        변환된 행렬 데이터를 원본 데이터 형식으로 복원합니다.\n",
    "        \n",
    "        입력 데이터 타입(np.array 또는 pd.DataFrame)에 따라 동일한 형식으로 반환합니다.\n",
    "        \n",
    "        Args:\n",
    "            data (np.array): CTGAN 학습에 사용된 행렬 데이터.\n",
    "            sigmas (list 또는 None): 복원 시 사용될 표준편차 값 (연속형 데이터에 한함).\n",
    "        \n",
    "        Returns:\n",
    "            원본 형식 (np.array 또는 pd.DataFrame)의 복원된 데이터.\n",
    "        \"\"\"\n",
    "        st = 0\n",
    "        recovered_column_data_list = []\n",
    "        column_names = []\n",
    "        # 각 컬럼별로 저장된 변환 정보를 사용하여 데이터 복원\n",
    "        for column_transform_info in self._column_transform_info_list:\n",
    "            dim = column_transform_info.output_dimensions\n",
    "            column_data = data[:, st:st + dim]\n",
    "            if column_transform_info.column_type == 'continuous':\n",
    "                recovered_column_data = self._inverse_transform_continuous(\n",
    "                    column_transform_info, column_data, sigmas, st\n",
    "                )\n",
    "            else:\n",
    "                recovered_column_data = self._inverse_transform_discrete(\n",
    "                    column_transform_info, column_data\n",
    "                )\n",
    "            recovered_column_data_list.append(recovered_column_data)\n",
    "            column_names.append(column_transform_info.column_name)\n",
    "            st += dim\n",
    "\n",
    "        # 복원된 각 컬럼 데이터를 하나의 배열로 결합 후 DataFrame으로 변환\n",
    "        recovered_data = np.column_stack(recovered_column_data_list)\n",
    "        recovered_data = (pd.DataFrame(recovered_data, columns=column_names)\n",
    "                          .astype(self._column_raw_dtypes))\n",
    "        # 원본 데이터가 DataFrame이 아니었다면, 넘파이 배열로 반환\n",
    "        if not self.dataframe:\n",
    "            recovered_data = recovered_data.to_numpy()\n",
    "\n",
    "        return recovered_data\n",
    "\n",
    "    def convert_column_name_value_to_id(self, column_name, value):\n",
    "        \"\"\"\n",
    "        주어진 컬럼 이름과 값에 해당하는 ID를 반환합니다.\n",
    "        \n",
    "        Args:\n",
    "            column_name (str): 조건을 지정할 컬럼 이름.\n",
    "            value (str 또는 해당 타입): 해당 컬럼에서 찾고자 하는 값.\n",
    "        \n",
    "        Returns:\n",
    "            dict:\n",
    "                {'discrete_column_id': int, 'column_id': int, 'value_id': int}\n",
    "                - discrete_column_id: 이산형 컬럼 내에서의 순서 번호.\n",
    "                - column_id: 전체 컬럼 리스트 내에서의 인덱스.\n",
    "                - value_id: One-Hot 인코딩 결과에서 가장 큰 값을 가지는 인덱스.\n",
    "        \n",
    "        예외:\n",
    "            주어진 컬럼 이름 또는 값이 존재하지 않으면 ValueError 발생.\n",
    "        \"\"\"\n",
    "        discrete_counter = 0\n",
    "        column_id = 0\n",
    "        # _column_transform_info_list를 순회하며 해당 컬럼을 찾음\n",
    "        for column_transform_info in self._column_transform_info_list:\n",
    "            if column_transform_info.column_name == column_name:\n",
    "                break\n",
    "            if column_transform_info.column_type == 'discrete':\n",
    "                discrete_counter += 1\n",
    "            column_id += 1\n",
    "        else:\n",
    "            raise ValueError(f\"The column_name `{column_name}` doesn't exist in the data.\")\n",
    "\n",
    "        # 해당 컬럼의 OneHotEncoder를 사용하여 주어진 값이 원-핫 인코딩된 결과를 얻음\n",
    "        ohe = column_transform_info.transform\n",
    "        data = pd.DataFrame([value], columns=[column_transform_info.column_name])\n",
    "        one_hot = ohe.transform(data).to_numpy()[0]\n",
    "        if sum(one_hot) == 0:\n",
    "            raise ValueError(f\"The value `{value}` doesn't exist in the column `{column_name}`.\")\n",
    "\n",
    "        return {\n",
    "            'discrete_column_id': discrete_counter,\n",
    "            'column_id': column_id,\n",
    "            'value_id': np.argmax(one_hot)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6R144Karphu"
   },
   "source": [
    "# G, D, Q 네트워크 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1716703010869,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "PDWuoVg1iXwe",
    "outputId": "f6a04b28-aec3-43fc-bbd9-f918fb45a0cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1]) torch.Size([100, 10]) torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Linear, LeakyReLU, Dropout, Sequential\n",
    "\n",
    "class Discriminator(Module):\n",
    "    \"\"\"\n",
    "    CTGAN용 Discriminator 클래스.\n",
    "    이 모델은 InfoGAN의 개념을 도입하여, D_head와 Q_head를 포함합니다.\n",
    "    - D_head: 진짜와 가짜를 판별하는 역할.\n",
    "    - Q_head: 잠재 코드(latent code)를 예측하여 정보 최대화(Mutual Information)를 유도합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, discriminator_dim, latent_dim, pac=10):\n",
    "        \"\"\"\n",
    "        Discriminator 초기화.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): 원본 데이터의 차원.\n",
    "            discriminator_dim (list): 은닉층의 노드 수 리스트.\n",
    "            latent_dim (int): Q_head에서 예측할 잠재 코드 차원.\n",
    "            pac (int): PacGAN에서 사용하는 패치 크기 (한 묶음에 포함할 샘플 수).\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        # PacGAN 기법을 적용하기 위해 입력 차원을 pac 배로 확장\n",
    "        dim = input_dim * pac\n",
    "        self.pac = pac\n",
    "        self.pacdim = dim\n",
    "\n",
    "        # D_head와 Q_head가 공유하는 신경망 계층 구성\n",
    "        seq = []\n",
    "        for item in list(discriminator_dim):\n",
    "            # 각 Linear 계층 뒤에 LeakyReLU와 Dropout을 적용\n",
    "            seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]\n",
    "            dim = item  # 출력 차원을 다음 계층의 입력 차원으로 사용\n",
    "\n",
    "        self.shared = Sequential(*seq)\n",
    "\n",
    "        # D_head: 진짜/가짜 판별을 위한 선형 계층 (출력 차원 1)\n",
    "        self.D_head = Linear(dim, 1)\n",
    "\n",
    "        # Q_head: 잠재 코드 예측을 위한 계층들\n",
    "        # Q_mu: 잠재 코드의 평균 예측\n",
    "        self.Q_mu = Linear(dim, latent_dim)\n",
    "        # Q_var: 잠재 코드의 분산 예측 (분산은 양수여야 하므로, forward에서 exp 함수를 적용)\n",
    "        self.Q_var = Linear(dim, latent_dim)\n",
    "\n",
    "    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n",
    "        \"\"\"\n",
    "        WGAN-GP를 위한 Gradient Penalty를 계산합니다.\n",
    "        \n",
    "        Args:\n",
    "            real_data (torch.Tensor): 실제 데이터.\n",
    "            fake_data (torch.Tensor): 생성된 가짜 데이터.\n",
    "            device (str): 사용할 디바이스 (예: 'cpu' 또는 'cuda').\n",
    "            pac (int): PacGAN에서 사용하는 샘플 묶음 크기.\n",
    "            lambda_ (float): Gradient Penalty 계수.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: 계산된 Gradient Penalty 값.\n",
    "        \"\"\"\n",
    "        # 각 pac 묶음에 대해 난수를 생성하여 보간(interpolation)에 사용할 가중치 생성\n",
    "        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n",
    "        alpha = alpha.repeat(1, pac, real_data.size(1))\n",
    "        alpha = alpha.view(-1, real_data.size(1))\n",
    "\n",
    "        # 실제 데이터와 가짜 데이터 사이의 선형 보간(interpolates) 계산\n",
    "        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "        # 보간된 데이터에 대해 Discriminator를 적용하여 출력 계산\n",
    "        disc_interpolates = self(interpolates)[0]\n",
    "\n",
    "        # 보간된 데이터에 대한 gradient 계산\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=disc_interpolates, \n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n",
    "            create_graph=True, \n",
    "            retain_graph=True, \n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        # gradient의 L2 norm 계산 후 1에서 빼고 제곱, 평균을 내어 Gradient Penalty 계산\n",
    "        gradients_view = gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n",
    "        gradient_penalty = ((gradients_view) ** 2).mean() * lambda_\n",
    "\n",
    "        return gradient_penalty\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Discriminator의 순전파 과정.\n",
    "        입력 데이터를 받아, D_head와 Q_head를 통해 진짜/가짜 판별 결과와 잠재 코드 예측 결과를 반환합니다.\n",
    "        \n",
    "        Args:\n",
    "            input_ (torch.Tensor): 입력 데이터.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (D_head 출력, Q_head에서 예측한 평균(mu), Q_head에서 예측한 분산(var))\n",
    "        \"\"\"\n",
    "        batch_size = input_.size(0)\n",
    "        # PacGAN 적용 시, 배치 크기가 pac의 배수인지 확인\n",
    "        assert batch_size % self.pac == 0\n",
    "\n",
    "        # pac 단위로 데이터를 묶어 하나의 긴 벡터로 변환\n",
    "        combined_input = input_.view(batch_size // self.pac, -1)\n",
    "        # 공유 계층을 통해 특징 추출\n",
    "        shared_output = self.shared(combined_input)\n",
    "\n",
    "        # D_head를 통해 진짜/가짜 판별 결과 계산\n",
    "        disc_output = self.D_head(shared_output)\n",
    "\n",
    "        # Q_head를 통해 잠재 코드의 평균과 분산 예측\n",
    "        mu = self.Q_mu(shared_output)\n",
    "        var = torch.exp(self.Q_var(shared_output))  # 분산은 항상 양수가 되어야 하므로 exp 적용\n",
    "\n",
    "        # Q_head의 예측 결과를 원래 배치 크기에 맞게 반복\n",
    "        mu = mu.repeat(self.pac, 1)\n",
    "        var = var.repeat(self.pac, 1)\n",
    "\n",
    "        # 최종적으로, D_head의 출력, 평균, 분산을 반환 (배치 사이즈에 맞게 reshape)\n",
    "        return disc_output, mu.view(batch_size, -1), var.view(batch_size, -1)\n",
    "\n",
    "# # 테스트 코드\n",
    "# # 설정 파라미터\n",
    "# input_dim = 128         # 입력 데이터 차원\n",
    "# discriminator_dim = [256, 128]  # 은닉층 크기 리스트\n",
    "# latent_dim = 10         # Q_head에서 예측할 잠재 코드 차원\n",
    "# pac = 10                # PacGAN에서 사용할 샘플 묶음 크기\n",
    "# batch_size = 100        # 배치 크기\n",
    "\n",
    "# # Discriminator 객체 생성\n",
    "# discriminator = Discriminator(input_dim, discriminator_dim, latent_dim, pac=pac)\n",
    "\n",
    "# # 테스트용 랜덤 데이터 생성\n",
    "# real_data = torch.randn(batch_size, input_dim)\n",
    "# fake_data = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# # Discriminator를 통해 실제 데이터에 대한 출력 확인\n",
    "# disc_output, mu, var = discriminator(real_data)\n",
    "# print(disc_output.shape, mu.shape, var.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1716703010869,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "G6uVsq8NSvke"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Linear, BatchNorm1d, ReLU\n",
    "\n",
    "class Residual(Module):\n",
    "    \"\"\"\n",
    "    CTGAN에서 사용되는 Residual Layer.\n",
    "    이 레이어는 입력 데이터와 잠재 코드(latent code)를 결합하여 처리한 후,\n",
    "    원본 입력을 skip connection 방식으로 최종 출력에 다시 추가하여 정보를 보존합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, i, o, latent_dim):\n",
    "        \"\"\"\n",
    "        Residual 레이어를 초기화합니다.\n",
    "        \n",
    "        Args:\n",
    "            i (int): 입력 데이터의 차원.\n",
    "            o (int): 이 레이어에서 생성될 출력 차원.\n",
    "            latent_dim (int): 잠재 코드의 차원.\n",
    "        \"\"\"\n",
    "        super(Residual, self).__init__()\n",
    "        # 입력 데이터와 잠재 코드를 결합한 후, o 차원으로 선형 변환\n",
    "        self.fc = Linear(i + latent_dim, o)\n",
    "        # 선형 변환의 출력을 정규화하여 학습 안정성을 높임\n",
    "        self.bn = BatchNorm1d(o)\n",
    "        # ReLU 활성화 함수 적용하여 비선형성을 부여\n",
    "        self.relu = ReLU()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, input_, latent_code):\n",
    "        \"\"\"\n",
    "        입력 데이터와 잠재 코드를 받아 Residual 연결을 적용합니다.\n",
    "        \n",
    "        Args:\n",
    "            input_ (torch.Tensor): 입력 데이터 텐서 (batch_size, input_dim).\n",
    "            latent_code (torch.Tensor): 잠재 코드 텐서 (batch_size, latent_dim).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Residual 연결이 적용된 결과 텐서.\n",
    "        \"\"\"\n",
    "        # latent_code를 입력 데이터의 배치 크기에 맞게 확장 (batch_size x latent_dim)\n",
    "        latent_code_expanded = latent_code.expand(input_.size(0), self.latent_dim)\n",
    "        # 입력 데이터와 확장된 잠재 코드를 열 방향으로 결합\n",
    "        combined_input = torch.cat([input_, latent_code_expanded], dim=1)\n",
    "        # 결합된 입력에 대해 선형 변환 -> 배치 정규화 -> ReLU 활성화 함수 적용\n",
    "        out = self.fc(combined_input)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        # 최종 출력은 선형 변환 결과와 원본 입력을 다시 결합하여, skip connection을 적용함\n",
    "        return torch.cat([out, input_], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1716703010869,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "VNJB-6zjSvg5"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "\n",
    "class Generator(Module):\n",
    "    \"\"\"\n",
    "    CTGAN에서 사용되는 Generator.\n",
    "    이 생성기는 입력된 노이즈 벡터(embedding)와 잠재 코드(latent code)를 결합하여\n",
    "    최종적으로 CTGAN이 생성할 데이터(data_dim 차원)를 출력합니다.\n",
    "    \n",
    "    Generator는 Residual Block을 반복적으로 쌓아 깊은 네트워크를 구성하며,\n",
    "    각 Residual Block은 입력 데이터와 잠재 코드 정보를 결합하여 처리한 후,\n",
    "    입력 정보를 skip connection 방식으로 추가합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, generator_dim, data_dim, latent_dim):\n",
    "        \"\"\"\n",
    "        Generator 초기화.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim (int): 입력 노이즈 벡터의 차원.\n",
    "            generator_dim (list or tuple): 각 Residual Block에서 사용할 은닉층의 출력 차원 리스트.\n",
    "            data_dim (int): 최종 생성할 데이터의 차원.\n",
    "            latent_dim (int): 잠재 코드의 차원.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        # 초기 입력 차원은 embedding_dim (노이즈 벡터 차원)\n",
    "        dim = embedding_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        seq = []  # Generator의 각 레이어를 순차적으로 저장할 리스트\n",
    "        \n",
    "        # generator_dim에 지정된 각 값에 대해 Residual Block을 추가\n",
    "        for item in list(generator_dim):\n",
    "            # Residual Block: 입력 차원(dim)과 출력 차원(item), 그리고 latent_dim을 사용\n",
    "            seq += [Residual(dim, item, latent_dim)]\n",
    "            # Residual 연결로 인해 출력 차원이 증가: 기존 입력에 item 만큼의 출력이 더해짐\n",
    "            dim += item\n",
    "        \n",
    "        # 마지막 Fully Connected Layer를 추가하여 최종 데이터 차원(data_dim)으로 변환\n",
    "        seq.append(Linear(dim, data_dim))\n",
    "        \n",
    "        # Sequential 컨테이너에 레이어들을 담아 순차적으로 적용할 수 있도록 함\n",
    "        self.seq = Sequential(*seq)\n",
    "\n",
    "    def forward(self, input_, latent_code):\n",
    "        \"\"\"\n",
    "        Generator의 순전파 과정.\n",
    "        \n",
    "        Args:\n",
    "            input_ (torch.Tensor): 입력 노이즈 벡터 (batch_size, embedding_dim).\n",
    "            latent_code (torch.Tensor): 잠재 코드 (batch_size, latent_dim).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: 생성된 데이터 (batch_size, data_dim).\n",
    "        \"\"\"\n",
    "        # 초기 입력으로 노이즈 벡터를 사용\n",
    "        data = input_\n",
    "        # Sequential 컨테이너에 저장된 모든 레이어를 순차적으로 적용\n",
    "        for layer in self.seq:\n",
    "            # Residual 레이어는 latent_code도 함께 입력해야 함\n",
    "            if isinstance(layer, Residual):\n",
    "                data = layer(data, latent_code)\n",
    "            else:\n",
    "                # 마지막 Fully Connected Layer 등 latent_code가 필요 없는 경우\n",
    "                data = layer(data)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1716703010870,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "_gQeDxZjSvdn"
   },
   "outputs": [],
   "source": [
    "def sample(self, n, condition_column=None, condition_value=None):\n",
    "    \"\"\"\n",
    "    훈련 데이터와 유사한 샘플 데이터를 생성합니다.\n",
    "    \n",
    "    특정 condition_column과 condition_value를 지정하면 해당 조건을 \n",
    "    반영하여 샘플링할 확률이 높아집니다.\n",
    "    \n",
    "    Args:\n",
    "        n (int): 생성할 행(row)의 개수.\n",
    "        condition_column (str): 조건을 적용할 이산형 컬럼 이름.\n",
    "        condition_value (str): condition_column에서 우선 발생시킬 범주 값.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray 또는 pandas.DataFrame: 원본 데이터와 유사한 생성된 샘플 데이터.\n",
    "    \"\"\"\n",
    "    # 조건 컬럼과 값이 지정된 경우, 해당 조건에 맞는 조건 벡터를 생성\n",
    "    if condition_column is not None and condition_value is not None:\n",
    "        # DataTransformer를 통해 컬럼 이름과 값에 해당하는 ID 정보를 얻음\n",
    "        condition_info = self._transformer.convert_column_name_value_to_id(\n",
    "            condition_column, condition_value)\n",
    "        # DataSampler를 사용해 해당 조건을 반영한 조건 벡터 생성\n",
    "        global_condition_vec = self._data_sampler.generate_cond_from_condition_column_info(\n",
    "            condition_info, self._batch_size)\n",
    "    else:\n",
    "        # 조건이 없으면 global_condition_vec는 None으로 설정\n",
    "        global_condition_vec = None\n",
    "\n",
    "    # 전체 n개의 샘플을 생성하기 위해 필요한 배치 반복 횟수 계산\n",
    "    steps = n // self._batch_size + 1\n",
    "    data = []  # 생성된 데이터 저장 리스트\n",
    "\n",
    "    for i in range(steps):\n",
    "        # 1. 노이즈 벡터 생성: 평균 0, 표준편차 1인 정규분포를 따르는 벡터 생성\n",
    "        mean = torch.zeros(self._batch_size, self._embedding_dim)\n",
    "        std = mean + 1\n",
    "        fakez = torch.normal(mean=mean, std=std).to(self._device)\n",
    "\n",
    "        # 2. 조건 벡터 적용:\n",
    "        #    조건 벡터가 지정되어 있으면 그 벡터를 복사,\n",
    "        #    그렇지 않으면 원본 데이터 분포를 반영한 조건 벡터 생성\n",
    "        if global_condition_vec is not None:\n",
    "            condvec = global_condition_vec.copy()\n",
    "        else:\n",
    "            condvec = self._data_sampler.sample_original_condvec(self._batch_size)\n",
    "\n",
    "        # 3. 조건 벡터가 존재할 경우, NumPy 배열을 PyTorch Tensor로 변환 후 노이즈 벡터와 결합\n",
    "        if condvec is not None:\n",
    "            c1 = torch.from_numpy(condvec).to(self._device)\n",
    "            fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "        # 4. Generator를 이용해 가짜 데이터 생성\n",
    "        fake = self._generator(fakez)\n",
    "        # 5. Generator 출력에 대해 활성화 함수를 적용하여 적절한 값으로 변환\n",
    "        fakeact = self._apply_activate(fake)\n",
    "        # 6. 생성된 데이터를 CPU로 옮기고 NumPy 배열로 변환하여 저장\n",
    "        data.append(fakeact.detach().cpu().numpy())\n",
    "\n",
    "    # 모든 배치의 데이터를 하나의 배열로 결합하고, n개의 샘플만 선택\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    data = data[:n]\n",
    "\n",
    "    # DataTransformer의 inverse_transform을 통해 생성된 데이터를 원본 데이터 형식으로 복원하여 반환\n",
    "    return self._transformer.inverse_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1716703010870,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "LRBtPXKHiISi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class MutualInformationLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    상호정보 손실(Mutual Information Loss) 모듈.\n",
    "    InfoGAN 구조에서 Q 네트워크가 잠재 변수(latent code)를 복원하도록 유도하기 위해,\n",
    "    실제 잠재 변수(x)와 Q 네트워크가 예측한 평균(mu) 및 분산(var) 사이의 음의 로그 가능도(NLL)를 계산합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MutualInformationLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, x, mu, var):\n",
    "        \"\"\"\n",
    "        실제 잠재 변수(x)와 Q 네트워크의 예측(mu, var) 간의 음의 로그 가능도(NLL)를 계산합니다.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): 실제 잠재 변수, shape: (batch_size, latent_dim)\n",
    "            mu (torch.Tensor): Q 네트워크가 예측한 평균, shape: (batch_size, latent_dim)\n",
    "            var (torch.Tensor): Q 네트워크가 예측한 분산, shape: (batch_size, latent_dim)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: 계산된 상호정보 손실 값 (스칼라)\n",
    "        \"\"\"\n",
    "        # 정규 분포의 로그 가능도(log-likelihood) 계산:\n",
    "        # 첫 번째 항: 분산에 대한 로그 항 (-0.5 * log(2π * var))\n",
    "        # 두 번째 항: (x - mu)^2 / (2 * var)\n",
    "        logli = -0.5 * (var.mul(2 * np.pi) + 1e-6).log() - (x - mu).pow(2).div(var.mul(2.0) + 1e-6)\n",
    "        \n",
    "        # 각 샘플에 대해 로그 가능도 합산 후 평균을 취하고, 부호를 반전하여 음의 로그 가능도(NLL)를 계산\n",
    "        nll = -(logli.sum(1).mean())\n",
    "        \n",
    "        return nll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1716703010870,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "g9h0Ulb3Usc6"
   },
   "outputs": [],
   "source": [
    "# CTGAN\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential, functional\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CTGAN(BaseSynthesizer):\n",
    "    \"\"\"\n",
    "    Conditional Table GAN Synthesizer 클래스.\n",
    "    \n",
    "    CTGAN은 테이블 데이터를 조건부로 생성하기 위한 GAN 모델입니다.\n",
    "    자세한 내용은 논문 \"Modeling Tabular data using Conditional GAN\"을 참고하세요.\n",
    "    \n",
    "    인자:\n",
    "        embedding_dim (int): Generator에 입력될 랜덤 샘플(노이즈 벡터)의 차원. (기본값: 128)\n",
    "        generator_dim (tuple or list of ints): Generator의 각 Residual Layer의 출력 차원. (기본값: (256, 256))\n",
    "        discriminator_dim (tuple or list of ints): Discriminator의 각 Linear Layer의 출력 차원. (기본값: (256, 256))\n",
    "        latent_dim (int): 추가로 사용할 잠재 코드의 차원. (기본값: 1)\n",
    "        generator_lr (float): Generator 학습률. (기본값: 2e-4)\n",
    "        generator_decay (float): Generator 가중치 감쇠. (기본값: 1e-6)\n",
    "        discriminator_lr (float): Discriminator 학습률. (기본값: 2e-4)\n",
    "        discriminator_decay (float): Discriminator 가중치 감쇠. (기본값: 1e-6)\n",
    "        batch_size (int): 각 학습 단계에서 처리할 샘플 수. (기본값: 500)\n",
    "        discriminator_steps (int): Generator 업데이트 당 Discriminator 업데이트 횟수. (기본값: 1)\n",
    "        log_frequency (bool): 이산형 변수 샘플링 시 로그 빈도 사용 여부. (기본값: True)\n",
    "        verbose (bool): 학습 진행 상황을 출력할지 여부. (기본값: False)\n",
    "        epochs (int): 총 학습 에폭 수. (기본값: 300)\n",
    "        pac (int): PacGAN에서 한 묶음으로 처리할 샘플 수. (기본값: 10)\n",
    "        cuda (bool): GPU 사용 여부. (기본값: True; GPU 사용 가능하지 않으면 CPU 사용)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=128, generator_dim=(256, 256), discriminator_dim=(256, 256),\n",
    "                 latent_dim=1, generator_lr=2e-4, generator_decay=1e-6, discriminator_lr=2e-4,\n",
    "                 discriminator_decay=1e-6, batch_size=500, discriminator_steps=1,\n",
    "                 log_frequency=True, verbose=False, epochs=300, pac=10, cuda=True):\n",
    "\n",
    "        # 배치 크기는 짝수여야 함을 확인\n",
    "        assert batch_size % 2 == 0\n",
    "\n",
    "        # 기본 하이퍼파라미터 설정\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._generator_dim = generator_dim\n",
    "        self._discriminator_dim = discriminator_dim\n",
    "        self._latent_dim = latent_dim  # 추가되는 잠재 코드 차원\n",
    "\n",
    "        self._generator_lr = generator_lr\n",
    "        self._generator_decay = generator_decay\n",
    "        self._discriminator_lr = discriminator_lr\n",
    "        self._discriminator_decay = discriminator_decay\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._discriminator_steps = discriminator_steps\n",
    "        self._log_frequency = log_frequency\n",
    "        self._verbose = verbose\n",
    "        self._epochs = epochs\n",
    "        self.pac = pac\n",
    "\n",
    "        # 디바이스 설정: cuda 인자와 GPU 사용 가능 여부에 따라 CPU 또는 GPU 선택\n",
    "        if not cuda or not torch.cuda.is_available():\n",
    "            device = 'cpu'\n",
    "        elif isinstance(cuda, str):\n",
    "            device = cuda\n",
    "        else:\n",
    "            device = 'cuda'\n",
    "\n",
    "        self._device = torch.device(device)\n",
    "\n",
    "        # 이후 학습 및 샘플링 시 사용할 객체들 초기화\n",
    "        self._transformer = None            # 데이터 전처리를 위한 DataTransformer\n",
    "        self._data_sampler = None           # 조건 벡터 및 샘플 데이터를 위한 DataSampler\n",
    "        self._generator = None              # 생성기 네트워크\n",
    "        self.loss_values = None             # 에폭별 손실 값 기록용 DataFrame\n",
    "        self._discriminator = None          # 판별기 네트워크\n",
    "        self._q_network = None              # Q 네트워크 (잠재 코드 예측용; InfoGAN 아이디어)\n",
    "        self.mutual_information_loss = MutualInformationLoss()  # 상호정보 손실 모듈\n",
    "\n",
    "    @staticmethod\n",
    "    def _gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):\n",
    "        \"\"\"\n",
    "        구블 소프트맥스(gumbel_softmax)의 불안정성을 완화하기 위한 함수.\n",
    "        \n",
    "        인자:\n",
    "            logits: 정규화되지 않은 로그 확률들 (..., num_features)\n",
    "            tau: 온도 하이퍼파라미터 (양수 스칼라)\n",
    "            hard (bool): True이면, 반환된 샘플을 one-hot 벡터로 만들지만 미분은 soft sample로 처리.\n",
    "            eps: 수치 안정성을 위한 작은 값.\n",
    "            dim: 소프트맥스를 적용할 차원 (기본값: -1)\n",
    "        \n",
    "        반환:\n",
    "            logits와 동일한 shape의 gumbel_softmax 샘플.\n",
    "        \"\"\"\n",
    "        for _ in range(10):\n",
    "            transformed = functional.gumbel_softmax(logits, tau=tau, hard=hard, eps=eps, dim=dim)\n",
    "            if not torch.isnan(transformed).any():\n",
    "                return transformed\n",
    "\n",
    "        raise ValueError('gumbel_softmax returning NaN.')\n",
    "\n",
    "    def _apply_activate(self, data):\n",
    "        \"\"\"\n",
    "        Generator의 출력 데이터에 대해 각 컬럼별 활성화 함수(tanh 또는 softmax)를 적용합니다.\n",
    "        \n",
    "        DataTransformer에 의해 설정된 output_info_list를 기반으로,\n",
    "        각 컬럼마다 적절한 활성화 함수를 적용하여 결과를 하나의 텐서로 결합합니다.\n",
    "        \"\"\"\n",
    "        data_t = []\n",
    "        st = 0\n",
    "        # 각 컬럼의 변환 정보에 따라 활성화 함수 적용\n",
    "        for column_info in self._transformer.output_info_list:\n",
    "            for span_info in column_info:\n",
    "                if span_info.activation_fn == 'tanh':\n",
    "                    ed = st + span_info.dim\n",
    "                    data_t.append(torch.tanh(data[:, st:ed]))\n",
    "                    st = ed\n",
    "                elif span_info.activation_fn == 'softmax':\n",
    "                    ed = st + span_info.dim\n",
    "                    transformed = self._gumbel_softmax(data[:, st:ed], tau=0.2)\n",
    "                    data_t.append(transformed)\n",
    "                    st = ed\n",
    "                else:\n",
    "                    raise ValueError(f'Unexpected activation function {span_info.activation_fn}.')\n",
    "        return torch.cat(data_t, dim=1)\n",
    "\n",
    "    def _cond_loss(self, data, c, m):\n",
    "        \"\"\"\n",
    "        고정된 이산형 컬럼에 대해 cross entropy 손실을 계산합니다.\n",
    "        \n",
    "        인자:\n",
    "            data: Generator 출력 데이터.\n",
    "            c: 조건 벡터.\n",
    "            m: 마스크 벡터 (어떤 컬럼이 선택되었는지 나타냄).\n",
    "        \n",
    "        반환:\n",
    "            계산된 조건부 손실 (cross entropy loss).\n",
    "        \"\"\"\n",
    "        loss = []\n",
    "        st = 0\n",
    "        st_c = 0\n",
    "        # DataTransformer의 output_info_list를 순회하며 이산형 컬럼인 경우 cross entropy 계산\n",
    "        for column_info in self._transformer.output_info_list:\n",
    "            for span_info in column_info:\n",
    "                if len(column_info) != 1 or span_info.activation_fn != 'softmax':\n",
    "                    # 이산형 컬럼이 아니면 스킵\n",
    "                    st += span_info.dim\n",
    "                else:\n",
    "                    ed = st + span_info.dim\n",
    "                    ed_c = st_c + span_info.dim\n",
    "                    tmp = functional.cross_entropy(\n",
    "                        data[:, st:ed],\n",
    "                        torch.argmax(c[:, st_c:ed_c], dim=1),\n",
    "                        reduction='none'\n",
    "                    )\n",
    "                    loss.append(tmp)\n",
    "                    st = ed\n",
    "                    st_c = ed_c\n",
    "\n",
    "        loss = torch.stack(loss, dim=1)  # 각 컬럼별 손실을 하나의 텐서로 결합\n",
    "\n",
    "        return (loss * m).sum() / data.size()[0]\n",
    "\n",
    "    def _validate_discrete_columns(self, train_data, discrete_columns):\n",
    "        \"\"\"\n",
    "        학습 데이터(train_data)에 지정한 discrete_columns(이산형 컬럼)가 존재하는지 확인합니다.\n",
    "        \n",
    "        인자:\n",
    "            train_data (numpy.ndarray 또는 pandas.DataFrame): 2차원 학습 데이터.\n",
    "            discrete_columns (list-like): 조건 벡터 생성을 위한 이산형 컬럼 목록.\n",
    "                - DataFrame인 경우 컬럼 이름 리스트.\n",
    "                - numpy array인 경우 컬럼 인덱스 리스트.\n",
    "        \n",
    "        예외:\n",
    "            유효하지 않은 컬럼이 발견되면 ValueError 발생.\n",
    "        \"\"\"\n",
    "        if isinstance(train_data, pd.DataFrame):\n",
    "            invalid_columns = set(discrete_columns) - set(train_data.columns)\n",
    "        elif isinstance(train_data, np.ndarray):\n",
    "            invalid_columns = []\n",
    "            for column in discrete_columns:\n",
    "                if column < 0 or column >= train_data.shape[1]:\n",
    "                    invalid_columns.append(column)\n",
    "        else:\n",
    "            raise TypeError('`train_data` should be either pd.DataFrame or np.array.')\n",
    "\n",
    "        if invalid_columns:\n",
    "            raise ValueError(f'Invalid columns found: {invalid_columns}')\n",
    "\n",
    "    @random_state\n",
    "    def fit(self, train_data, discrete_columns=(), epochs=None):\n",
    "        \"\"\"\n",
    "        CTGAN 모델을 학습하기 위해 학습 데이터를 전처리하고, \n",
    "        Generator, Discriminator, DataSampler 등 관련 구성 요소들을 초기화합니다.\n",
    "        \n",
    "        Args:\n",
    "            train_data (numpy.ndarray 또는 pandas.DataFrame): 2차원 학습 데이터.\n",
    "            discrete_columns (list-like): 조건 벡터 생성을 위한 이산형 컬럼 목록.\n",
    "            epochs: 학습 에폭 수. (생성자에서 지정한 값을 기본으로 사용)\n",
    "        \n",
    "        동작:\n",
    "            1. discrete_columns의 유효성 검사.\n",
    "            2. DataTransformer를 사용하여 데이터를 변환.\n",
    "            3. DataSampler 생성.\n",
    "            4. Generator와 Discriminator 네트워크 생성 및 옵티마이저 설정.\n",
    "            5. 각 에폭마다 Generator와 Discriminator를 번갈아 업데이트하며 학습.\n",
    "            6. 에폭별 손실값을 기록.\n",
    "        \"\"\"\n",
    "        # 이산형 컬럼이 학습 데이터에 존재하는지 검증\n",
    "        self._validate_discrete_columns(train_data, discrete_columns)\n",
    "\n",
    "        if epochs is None:\n",
    "            epochs = self._epochs\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                ('`epochs` argument in `fit` method has been deprecated and will be removed '\n",
    "                 'in a future version. Please pass `epochs` to the constructor instead'),\n",
    "                DeprecationWarning\n",
    "            )\n",
    "\n",
    "        # 데이터 전처리: DataTransformer를 사용하여 원본 데이터를 변환\n",
    "        self._transformer = DataTransformer()\n",
    "        self._transformer.fit(train_data, discrete_columns)\n",
    "        train_data = self._transformer.transform(train_data)\n",
    "\n",
    "        # DataSampler 생성: 조건 벡터 및 샘플 데이터를 위한 객체\n",
    "        self._data_sampler = DataSampler(\n",
    "            train_data,\n",
    "            self._transformer.output_info_list,\n",
    "            self._log_frequency)\n",
    "\n",
    "        data_dim = self._transformer.output_dimensions\n",
    "\n",
    "        # Generator 생성 (latent code 추가)\n",
    "        self._generator = Generator(\n",
    "            self._embedding_dim + self._data_sampler.dim_cond_vec(),\n",
    "            self._generator_dim,\n",
    "            data_dim,\n",
    "            self._latent_dim  # latent_dim 추가\n",
    "        ).to(self._device)\n",
    "\n",
    "        # Discriminator 생성 (latent code 추가)\n",
    "        self._discriminator = Discriminator(\n",
    "            data_dim + self._data_sampler.dim_cond_vec(),\n",
    "            self._discriminator_dim,\n",
    "            latent_dim=self._latent_dim,  # latent_dim 추가\n",
    "            pac=self.pac\n",
    "        ).to(self._device)\n",
    "\n",
    "        # 옵티마이저 설정: Adam 옵티마이저 사용\n",
    "        optimizerG = optim.Adam(\n",
    "            self._generator.parameters(), lr=self._generator_lr, betas=(0.5, 0.9),\n",
    "            weight_decay=self._generator_decay\n",
    "        )\n",
    "\n",
    "        optimizerD = optim.Adam(\n",
    "            self._discriminator.parameters(), lr=self._discriminator_lr,\n",
    "            betas=(0.5, 0.9), weight_decay=self._discriminator_decay\n",
    "        )\n",
    "\n",
    "        # 노이즈 벡터 생성을 위한 평균과 표준편차 텐서 생성\n",
    "        mean = torch.zeros(self._batch_size, self._embedding_dim, device=self._device)\n",
    "        std = torch.ones(self._batch_size, self._embedding_dim, device=self._device)\n",
    "\n",
    "        # 에폭별 손실값을 기록할 DataFrame 초기화\n",
    "        self.loss_values = pd.DataFrame(columns=['Epoch', 'Generator Loss', 'Discriminator Loss'])\n",
    "\n",
    "        # 학습 진행률 표시를 위한 tqdm 이터레이터 설정\n",
    "        epoch_iterator = tqdm(range(epochs), disable=(not self._verbose))\n",
    "        if self._verbose:\n",
    "            description = 'Gen. ({gen:.2f}) | Discrim. ({dis:.2f})'\n",
    "            epoch_iterator.set_description(description.format(gen=0, dis=0))\n",
    "\n",
    "        # 에폭 당 스텝 수 계산 (전체 데이터를 배치 크기로 나눈 값, 최소 1)\n",
    "        steps_per_epoch = max(len(train_data) // self._batch_size, 1)\n",
    "        for i in epoch_iterator:\n",
    "            for id_ in range(steps_per_epoch):\n",
    "\n",
    "                # Discriminator 업데이트: 지정된 횟수만큼 반복\n",
    "                for n in range(self._discriminator_steps):\n",
    "                    fakez = torch.normal(mean=mean, std=std)\n",
    "                    # Uniform 분포에서 latent code 생성 (범위: -1 ~ 1)\n",
    "                    latent_code = torch.rand(self._batch_size, self._latent_dim, device=self._device) * 2 - 1  \n",
    "\n",
    "                    # 조건 벡터 샘플링\n",
    "                    condvec = self._data_sampler.sample_condvec(self._batch_size)\n",
    "                    if condvec is None:\n",
    "                        c1, m1, col, opt = None, None, None, None\n",
    "                        real = self._data_sampler.sample_data(\n",
    "                            train_data, self._batch_size, col, opt)\n",
    "                    else:\n",
    "                        c1, m1, col, opt = condvec\n",
    "                        c1 = torch.from_numpy(c1).to(self._device)\n",
    "                        m1 = torch.from_numpy(m1).to(self._device)\n",
    "                        fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "                        # 데이터 샘플의 순서를 무작위로 섞음\n",
    "                        perm = np.arange(self._batch_size)\n",
    "                        np.random.shuffle(perm)\n",
    "                        real = self._data_sampler.sample_data(\n",
    "                            train_data, self._batch_size, col[perm], opt[perm])\n",
    "                        c2 = c1[perm]\n",
    "                    # Generator에 노이즈와 latent code (및 조건 벡터) 입력하여 가짜 데이터 생성\n",
    "                    fake = self._generator(fakez, latent_code)  # latent code 추가\n",
    "                    fakeact = self._apply_activate(fake)\n",
    "\n",
    "                    real = torch.from_numpy(real.astype('float32')).to(self._device)\n",
    "\n",
    "                    if c1 is not None:\n",
    "                        fake_cat = torch.cat([fakeact, c1], dim=1)\n",
    "                        real_cat = torch.cat([real, c2], dim=1)\n",
    "                    else:\n",
    "                        real_cat = real\n",
    "                        fake_cat = fakeact\n",
    "\n",
    "                    # Discriminator를 통한 진짜/가짜 판별 결과 계산\n",
    "                    y_fake, _, _ = self._discriminator(fake_cat)\n",
    "                    y_real, _, _ = self._discriminator(real_cat)\n",
    "\n",
    "                    # Gradient Penalty 계산 (WGAN-GP)\n",
    "                    pen = self._discriminator.calc_gradient_penalty(\n",
    "                        real_cat, fake_cat, self._device, self.pac)\n",
    "                    loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n",
    "\n",
    "                    optimizerD.zero_grad(set_to_none=False)\n",
    "                    pen.backward(retain_graph=True)\n",
    "                    loss_d.backward()\n",
    "                    optimizerD.step()\n",
    "\n",
    "                # Generator 업데이트 단계\n",
    "                fakez = torch.normal(mean=mean, std=std)\n",
    "                latent_code = torch.rand(self._batch_size, self._latent_dim, device=self._device) * 2 - 1  # latent code 생성\n",
    "                condvec = self._data_sampler.sample_condvec(self._batch_size)\n",
    "\n",
    "                if condvec is None:\n",
    "                    c1, m1, col, opt = None, None, None, None\n",
    "                else:\n",
    "                    c1, m1, col, opt = condvec\n",
    "                    c1 = torch.from_numpy(c1).to(self._device)\n",
    "                    m1 = torch.from_numpy(m1).to(self._device)\n",
    "                    fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "                fake = self._generator(fakez, latent_code)  # latent code 추가\n",
    "                fakeact = self._apply_activate(fake)\n",
    "\n",
    "                if c1 is not None:\n",
    "                    y_fake, mu, var = self._discriminator(torch.cat([fakeact, c1], dim=1))\n",
    "                else:\n",
    "                    y_fake, mu, var = self._discriminator(fakeact)\n",
    "\n",
    "                if condvec is None:\n",
    "                    cross_entropy = 0\n",
    "                else:\n",
    "                    cross_entropy = self._cond_loss(fake, c1, m1)\n",
    "\n",
    "                # Mutual Information Loss 계산: latent code와 Discriminator의 Q_head 예측 간 차이\n",
    "                mi_loss = self.mutual_information_loss(latent_code, mu, var)  \n",
    "\n",
    "                # Generator 손실: -Discriminator 출력의 평균 + cross entropy + mutual information loss\n",
    "                loss_g = -torch.mean(y_fake) + cross_entropy + mi_loss  \n",
    "\n",
    "                optimizerG.zero_grad(set_to_none=False)\n",
    "                loss_g.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "            # 에폭별 손실값 기록\n",
    "            generator_loss = loss_g.detach().cpu().item()\n",
    "            discriminator_loss = loss_d.detach().cpu().item()\n",
    "\n",
    "            epoch_loss_df = pd.DataFrame({\n",
    "                'Epoch': [i],\n",
    "                'Generator Loss': [generator_loss],\n",
    "                'Discriminator Loss': [discriminator_loss]\n",
    "            })\n",
    "            if not self.loss_values.empty:\n",
    "                self.loss_values = pd.concat(\n",
    "                    [self.loss_values, epoch_loss_df]\n",
    "                ).reset_index(drop=True)\n",
    "            else:\n",
    "                self.loss_values = epoch_loss_df\n",
    "\n",
    "            if self._verbose:\n",
    "                epoch_iterator.set_description(\n",
    "                    description.format(gen=generator_loss, dis=discriminator_loss)\n",
    "                )\n",
    "\n",
    "    def sample(self, n, condition_column=None, condition_value=None):\n",
    "        \"\"\"\n",
    "        학습된 모델을 사용하여 원본 데이터와 유사한 데이터를 샘플링합니다.\n",
    "        \n",
    "        조건으로 condition_column과 condition_value를 지정하면 해당 범주가 반영될 확률이 높아집니다.\n",
    "        \n",
    "        Args:\n",
    "            n (int): 생성할 샘플 행(row)의 개수.\n",
    "            condition_column (str): 조건을 적용할 이산형 컬럼 이름.\n",
    "            condition_value (str): condition_column에서 우선 발생시킬 범주 값.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray 또는 pandas.DataFrame: 생성된 샘플 데이터.\n",
    "        \"\"\"\n",
    "        # 조건이 지정된 경우: 해당 조건을 반영한 조건 벡터 생성\n",
    "        if condition_column is not None and condition_value is not None:\n",
    "            condition_info = self._transformer.convert_column_name_value_to_id(\n",
    "                condition_column, condition_value)\n",
    "            global_condition_vec = self._data_sampler.generate_cond_from_condition_column_info(\n",
    "                condition_info, self._batch_size)\n",
    "        else:\n",
    "            global_condition_vec = None\n",
    "\n",
    "        # n개의 샘플 생성을 위해 필요한 배치 수 계산\n",
    "        steps = n // self._batch_size + 1\n",
    "        data = []\n",
    "        for i in range(steps):\n",
    "            # 노이즈 벡터 생성: 평균 0, 표준편차 1인 정규분포 따름\n",
    "            mean = torch.zeros(self._batch_size, self._embedding_dim)\n",
    "            std = mean + 1\n",
    "            fakez = torch.normal(mean=mean, std=std).to(self._device)\n",
    "            # latent code 생성: 균등 분포, 범위 -1 ~ 1\n",
    "            latent_code = torch.rand(self._batch_size, self._latent_dim, device=self._device) * 2 - 1  \n",
    "\n",
    "            # 조건 벡터 적용: 지정된 조건 벡터가 있으면 복사, 없으면 원본 데이터 분포 기반 조건 벡터 샘플링\n",
    "            if global_condition_vec is not None:\n",
    "                condvec = global_condition_vec.copy()\n",
    "            else:\n",
    "                condvec = self._data_sampler.sample_original_condvec(self._batch_size)\n",
    "\n",
    "            # 조건 벡터가 존재하는 경우, 이를 노이즈 벡터에 결합\n",
    "            if condvec is not None:\n",
    "                c1 = condvec\n",
    "                c1 = torch.from_numpy(c1).to(self._device)\n",
    "                fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "            # Generator를 이용해 가짜 데이터 생성 (latent code 추가)\n",
    "            fake = self._generator(fakez, latent_code)\n",
    "            # Generator 출력에 활성화 함수 적용\n",
    "            fakeact = self._apply_activate(fake)\n",
    "            # 생성된 결과를 CPU로 옮기고 NumPy 배열로 변환 후 저장\n",
    "            data.append(fakeact.detach().cpu().numpy())\n",
    "\n",
    "        # 배치별 생성된 데이터를 하나의 배열로 결합하고, n개의 샘플만 선택\n",
    "        data = np.concatenate(data, axis=0)\n",
    "        data = data[:n]\n",
    "\n",
    "        # DataTransformer의 inverse_transform을 통해 원본 데이터 형식으로 복원하여 반환\n",
    "        return self._transformer.inverse_transform(data)\n",
    "\n",
    "    def set_device(self, device):\n",
    "        \"\"\"\n",
    "        모델에서 사용할 디바이스(GPU 또는 CPU)를 설정합니다.\n",
    "        \n",
    "        인자:\n",
    "            device: 사용할 디바이스 ('GPU' 또는 'CPU' 혹은 해당 문자열)\n",
    "        \n",
    "        동작:\n",
    "            Generator, Discriminator, Q 네트워크가 존재하는 경우 해당 디바이스로 이동시킵니다.\n",
    "        \"\"\"\n",
    "        self._device = device\n",
    "        if self._generator is not None:\n",
    "            self._generator.to(self._device)\n",
    "        if self._discriminator is not None:\n",
    "            self._discriminator.to(self._device)\n",
    "        if self._q_network is not None:\n",
    "            self._q_network.to(self._device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1716703010870,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "JPo0E-NzUdyY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data=pd.read_csv(\"/content/adult.csv\")\n",
    "# 종속변수는 그냥 0,1로 설정\n",
    "data['income'] = data['income'].replace({' <=50K': 0, ' >50K': 1})\n",
    "\n",
    "# 데이터 프레임을 훈련용과 테스트용으로 분리\n",
    "train_df, test_df = train_test_split(real_data, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# 훈련용 데이터로 샘플 생성\n",
    "# Names of the columns that are discrete\n",
    "discrete_columns = [\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native-country',\n",
    "    'income'\n",
    "]\n",
    "\n",
    "# CTGAN 모델 초기화\n",
    "embedding_dim = 128\n",
    "generator_dim = (256, 256)\n",
    "discriminator_dim = (256, 256)\n",
    "latent_dim = 1  # latent code의 차원\n",
    "batch_size = 500\n",
    "epochs = 100\n",
    "cuda = True\n",
    "pac = 10  # pac 값을 batch_size의 약수로 설정\n",
    "\n",
    "ctgan = CTGAN(\n",
    "    embedding_dim=embedding_dim,\n",
    "    generator_dim=generator_dim,\n",
    "    discriminator_dim=discriminator_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    pac=pac,  # pac 값을 설정\n",
    "    cuda=cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1716704531809,
     "user": {
      "displayName": "허주혁",
      "userId": "07084138917445903099"
     },
     "user_tz": -540
    },
    "id": "hAdlE8BSVQyb"
   },
   "outputs": [],
   "source": [
    "ctgan.fit(train_df, discrete_columns)\n",
    "# Create synthetic data\n",
    "synthetic_data = ctgan.sample(10000)\n",
    "\n",
    "# 범주형 컬럼 get dummies 사용\n",
    "synthetic_data_get_dummies=pd.get_dummies(synthetic_data)\n",
    "test_df_get_dummies=pd.get_dummies(test_df)\n",
    "\n",
    "X_train=synthetic_data_get_dummies.drop('income',axis=1)\n",
    "y_train=synthetic_data_get_dummies['income']\n",
    "\n",
    "X_test=test_df_get_dummies.drop('income',axis=1)\n",
    "y_test=test_df_get_dummies['income']\n",
    "\n",
    "\n",
    "# 범주형 컬럼에서 빈도가 적은 카테고리가 테스트에 포함되지 않을수도 있으니 그 카테고리 False로 차원 맞춰주기\n",
    "if X_train.shape[1]>X_test.shape[1]:\n",
    "  synthetic_data_get_dummies, test_df_get_dummies = synthetic_data_get_dummies.align(test_df_get_dummies, join='outer', axis=1, fill_value=False)\n",
    "elif X_train.shape[1]<X_test.shape[1]:\n",
    "  test_df_get_dummies, synthetic_data_get_dummies = test_df_get_dummies.align(synthetic_data_get_dummies, join='outer', axis=1, fill_value=False)\n",
    "else:\n",
    "  synthetic_data_get_dummies=pd.get_dummies(synthetic_data)\n",
    "  test_df_get_dummies=pd.get_dummies(test_df)\n",
    "\n",
    "\n",
    "X_train=synthetic_data_get_dummies.drop('income',axis=1)\n",
    "y_train=synthetic_data_get_dummies['income']\n",
    "\n",
    "X_test=test_df_get_dummies.drop('income',axis=1)\n",
    "y_test=test_df_get_dummies['income']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# 의사결정 나무 모델 초기화 및 학습\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# latent_dim : 1, epochs : 100\n",
    "# F1 스코어 계산\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 스코어:\", f1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOeidyeqfVTxOIIyq7iwIiv",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
